{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\pc-bae-2\\AppData\\Local\\Temp\\tmpb62i0hl2\\config.json as plain json\n",
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data import DataLoader\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import PretrainedTransformerMismatchedEmbedder\n",
    "from allennlp.data import Instance\n",
    "from allennlp.models.archival import load_archive\n",
    "\n",
    "# Load the pre-trained archive\n",
    "archive = load_archive(\"spanbert_local/coref-spanbert-large-2021.03.10.tar.gz\")\n",
    "model = archive.model\n",
    "# Make sure to move the model to the correct device\n",
    "model = model.cuda(0)  # Replace with model.cpu() if you don't want to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_reader.Params({'type': 'coref', 'max_sentences': 110, 'max_span_width': 30, 'token_indexers': {'tokens': {'type': 'pretrained_transformer_mismatched', 'max_length': 512, 'model_name': 'SpanBERT/spanbert-large-cased'}}})\n"
     ]
    }
   ],
   "source": [
    "print(archive.config[\"dataset_reader\"])\n",
    "\n",
    "train_path = \"gap-development.jsonl\"  \n",
    "val_path = \"gap-validation.jsonl\" \n",
    "# Load your training and validation data\n",
    "reader = DatasetReader.from_params(archive.config[\"dataset_reader\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # # You can extend the vocabulary here if needed\n",
    "# # vocab = model.vocab\n",
    "# # vocab.extend_from_instances(train_data)\n",
    "# # vocab.extend_from_instances(validation_data)\n",
    "\n",
    "# # Create data loaders\n",
    "# from allennlp.data.data_loaders import MultiProcessDataLoader \n",
    "# from allennlp.data.samplers import BucketBatchSampler\n",
    "# from allennlp.data import Vocabulary\n",
    "\n",
    "\n",
    "\n",
    "# # Make sure your model also uses the same vocabulary\n",
    "# vocab = model.vocab\n",
    "\n",
    "# train_sampler = BucketBatchSampler(batch_size=4, sorting_keys=[\"tokens\"])\n",
    "# print(val_path)\n",
    "# train_data_loader = MultiProcessDataLoader(reader=reader,data_path=train_path, batch_sampler=train_sampler)\n",
    "# validation_data_loader = MultiProcessDataLoader(reader=reader,data_path=val_path, batch_size=4)\n",
    "\n",
    "# # Index the data loaders with the vocabulary\n",
    "# train_data_loader.index_with(vocab)\n",
    "# validation_data_loader.index_with(vocab)\n",
    "\n",
    "# # Make sure your model also uses the same vocabulary\n",
    "# model.vocab = vocab\n",
    "# print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data import DatasetReader, Instance, Field\n",
    "from allennlp.data.fields import TextField, SpanField, ListField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer, SpacyTokenizer\n",
    "from typing import Iterator, List, Dict, Any\n",
    "import json\n",
    "\n",
    "class CustomCorefReader(DatasetReader):\n",
    "    def __init__(self, \n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 **kwargs: Any):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer = tokenizer or SpacyTokenizer()\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                example = json.loads(line)\n",
    "                text = example['text']\n",
    "                clusters = example['clusters']\n",
    "\n",
    "                tokens_with_offsets = self.tokenizer.tokenize(text)\n",
    "                tokens = [token.text for token in tokens_with_offsets]\n",
    "                token_offsets = [(token.idx, token.idx + len(token.text)) for token in tokens_with_offsets]\n",
    "\n",
    "                text_field = TextField(tokens_with_offsets, self.token_indexers)\n",
    "                \n",
    "                # Convert character-based spans to token-based spans\n",
    "                token_based_clusters = []\n",
    "                for cluster in clusters:\n",
    "                    token_based_cluster = []\n",
    "                    for char_start, char_end in cluster:\n",
    "                        try:\n",
    "                            token_start = next(i for i, (t_start, t_end) in enumerate(token_offsets) if t_start <= char_start < t_end)\n",
    "                        except StopIteration:\n",
    "                            print(f\"StopIteration for token_start: char_start={char_start}, token_offsets={token_offsets}\")\n",
    "                            raise\n",
    "\n",
    "                        try:\n",
    "                            token_end = next(i for i, (t_start, t_end) in enumerate(token_offsets) if t_start < char_end <= t_end)\n",
    "                        except StopIteration as e:\n",
    "                            print(f\"StopIteration for token_end: char_end={char_end}, token_offsets={token_offsets}\")\n",
    "                            # Assuming `text` contains the actual text content.\n",
    "                            print(f\"Text around char_end: {text[char_end-10:char_end+10]}\")\n",
    "\n",
    "                            continue\n",
    "\n",
    "                        token_based_cluster.append((token_start, token_end))\n",
    "                    token_based_clusters.append(token_based_cluster)\n",
    "\n",
    "                span_fields: List[Field] = []\n",
    "                for token_based_cluster in token_based_clusters:\n",
    "                    for start, end in token_based_cluster:\n",
    "                        span_fields.append(SpanField(start, end, text_field))\n",
    "\n",
    "                span_list_field = ListField(span_fields)\n",
    "            \n",
    "                fields: Dict[str, Field] = {'text': text_field, 'spans': span_list_field}\n",
    "                yield Instance(fields)\n",
    "\n",
    "reader = CustomCorefReader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopIteration for token_end: char_end=323, token_offsets=[(0, 2), (3, 12), (13, 15), (16, 24), (25, 31), (32, 34), (35, 42), (43, 48), (48, 49), (50, 51), (51, 52), (52, 55), (56, 61), (62, 67), (68, 70), (71, 73), (74, 77), (78, 85), (86, 92), (93, 97), (98, 104), (105, 107), (108, 114), (114, 116), (117, 127), (128, 134), (135, 143), (144, 146), (147, 150), (151, 157), (158, 166), (167, 169), (170, 179), (180, 188), (189, 191), (192, 195), (196, 201), (202, 204), (205, 206), (207, 211), (212, 216), (216, 217), (218, 224), (225, 235), (236, 243), (244, 251), (252, 254), (255, 263), (264, 269), (269, 270), (271, 279), (280, 283), (284, 295), (295, 296), (297, 301), (302, 304), (305, 308), (309, 316), (316, 317), (318, 322), (323, 324), (325, 329), (330, 333), (334, 343), (344, 347), (348, 361), (362, 366), (367, 370), (371, 380), (381, 385), (386, 388), (389, 400), (401, 407), (407, 409), (410, 414), (415, 417), (418, 420), (421, 426), (427, 430), (431, 437), (438, 440), (441, 444), (445, 450), (450, 451), (451, 453)]\n",
      "Text around char_end: ery, Jack * fans are\n",
      "StopIteration for token_end: char_end=285, token_offsets=[(0, 8), (9, 12), (13, 14), (15, 28), (29, 35), (36, 38), (39, 44), (45, 51), (52, 55), (56, 58), (59, 63), (64, 69), (70, 73), (74, 77), (78, 82), (83, 85), (86, 89), (90, 93), (94, 101), (101, 102), (103, 106), (107, 111), (112, 116), (116, 118), (119, 124), (125, 133), (134, 141), (141, 142), (143, 145), (146, 153), (154, 160), (161, 163), (164, 173), (174, 176), (177, 180), (181, 185), (185, 186), (187, 192), (193, 202), (203, 207), (208, 210), (211, 216), (216, 217), (218, 225), (226, 235), (235, 236), (237, 239), (240, 243), (244, 247), (248, 257), (258, 260), (261, 266), (266, 267), (268, 273), (274, 279), (280, 284), (285, 287), (288, 295), (296, 298), (299, 304), (305, 313), (313, 314), (315, 319), (320, 323), (324, 335), (336, 338), (339, 347), (348, 350), (351, 357), (358, 364), (364, 365), (366, 379), (380, 382), (383, 390), (391, 394), (395, 397), (398, 404), (404, 405), (406, 409), (410, 420), (421, 423), (424, 429), (429, 430), (431, 434), (435, 438), (439, 448), (449, 453), (454, 457), (458, 463), (464, 466), (467, 468), (469, 474), (475, 480), (480, 481), (482, 485), (486, 491), (492, 498), (499, 502), (503, 509), (510, 513), (514, 523), (524, 534), (535, 537), (538, 539), (540, 548), (549, 554), (554, 555), (556, 563), (564, 570), (571, 574), (575, 584), (585, 595), (595, 596)]\n",
      "Text around char_end: amed John C. Calhoun\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36369d3e32d1434ba25a3703d58cc0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43372f3f8adb4c2391ced1fbc3560032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopIteration for token_end: char_end=323, token_offsets=[(0, 2), (3, 12), (13, 15), (16, 24), (25, 31), (32, 34), (35, 42), (43, 48), (48, 49), (50, 51), (51, 52), (52, 55), (56, 61), (62, 67), (68, 70), (71, 73), (74, 77), (78, 85), (86, 92), (93, 97), (98, 104), (105, 107), (108, 114), (114, 116), (117, 127), (128, 134), (135, 143), (144, 146), (147, 150), (151, 157), (158, 166), (167, 169), (170, 179), (180, 188), (189, 191), (192, 195), (196, 201), (202, 204), (205, 206), (207, 211), (212, 216), (216, 217), (218, 224), (225, 235), (236, 243), (244, 251), (252, 254), (255, 263), (264, 269), (269, 270), (271, 279), (280, 283), (284, 295), (295, 296), (297, 301), (302, 304), (305, 308), (309, 316), (316, 317), (318, 322), (323, 324), (325, 329), (330, 333), (334, 343), (344, 347), (348, 361), (362, 366), (367, 370), (371, 380), (381, 385), (386, 388), (389, 400), (401, 407), (407, 409), (410, 414), (415, 417), (418, 420), (421, 426), (427, 430), (431, 437), (438, 440), (441, 444), (445, 450), (450, 451), (451, 453)]\n",
      "Text around char_end: ery, Jack * fans are\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dee142db8334a2e8f0088b7e6ee29ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopIteration for token_end: char_end=285, token_offsets=[(0, 8), (9, 12), (13, 14), (15, 28), (29, 35), (36, 38), (39, 44), (45, 51), (52, 55), (56, 58), (59, 63), (64, 69), (70, 73), (74, 77), (78, 82), (83, 85), (86, 89), (90, 93), (94, 101), (101, 102), (103, 106), (107, 111), (112, 116), (116, 118), (119, 124), (125, 133), (134, 141), (141, 142), (143, 145), (146, 153), (154, 160), (161, 163), (164, 173), (174, 176), (177, 180), (181, 185), (185, 186), (187, 192), (193, 202), (203, 207), (208, 210), (211, 216), (216, 217), (218, 225), (226, 235), (235, 236), (237, 239), (240, 243), (244, 247), (248, 257), (258, 260), (261, 266), (266, 267), (268, 273), (274, 279), (280, 284), (285, 287), (288, 295), (296, 298), (299, 304), (305, 313), (313, 314), (315, 319), (320, 323), (324, 335), (336, 338), (339, 347), (348, 350), (351, 357), (358, 364), (364, 365), (366, 379), (380, 382), (383, 390), (391, 394), (395, 397), (398, 404), (404, 405), (406, 409), (410, 420), (421, 423), (424, 429), (429, 430), (431, 434), (435, 438), (439, 448), (449, 453), (454, 457), (458, 463), (464, 466), (467, 468), (469, 474), (475, 480), (480, 481), (482, 485), (486, 491), (492, 498), (499, 502), (503, 509), (510, 513), (514, 523), (524, 534), (535, 537), (538, 539), (540, 548), (549, 554), (554, 555), (556, 563), (564, 570), (571, 574), (575, 584), (585, 595), (595, 596)]\n",
      "Text around char_end: amed John C. Calhoun\n",
      "Number of instances in train_data: 2000\n",
      "Number of instances in validation_data: 454\n"
     ]
    }
   ],
   "source": [
    "# Read the training and validation data\n",
    "train_data = list(reader.read(train_path))\n",
    "validation_data = list(reader.read(val_path))\n",
    "print(len(train_data))\n",
    "# Build a vocabulary from the training data\n",
    "vocab = Vocabulary.from_instances(train_data)\n",
    "\n",
    "# Create data loaders\n",
    "from allennlp.data.data_loaders import MultiProcessDataLoader\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "\n",
    "train_sampler = BucketBatchSampler(batch_size=4, sorting_keys=[\"text\"])\n",
    "\n",
    "train_data_loader = MultiProcessDataLoader(reader=reader, data_path=train_path, batch_sampler=train_sampler)\n",
    "validation_data_loader = MultiProcessDataLoader(reader=reader, data_path=val_path, batch_size=4)\n",
    "\n",
    "# Index the data loaders with the vocabulary\n",
    "train_data_loader.index_with(vocab)\n",
    "validation_data_loader.index_with(vocab)\n",
    "\n",
    "# Make sure your model also uses the same vocabulary\n",
    "model.vocab = vocab\n",
    "\n",
    "# Debugging: Print the number of instances in the data\n",
    "print(\"Number of instances in train_data:\", len(train_data))\n",
    "print(\"Number of instances in validation_data:\", len(validation_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
      "f:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\cuda\\memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9064376e0e0547b5a73cb5b79637fa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "PretrainedTransformerMismatchedEmbedder.forward() got an unexpected keyword argument 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\gap-coreference-master\\train_spanbert.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Create the trainer and train \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m trainer \u001b[39m=\u001b[39m GradientDescentTrainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     cuda_device\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m  \u001b[39m# set to -1 to use CPU\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\training\\gradient_descent_trainer.py:771\u001b[0m, in \u001b[0;36mGradientDescentTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    768\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     metrics, epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_train()\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m metrics\n\u001b[0;32m    773\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\training\\gradient_descent_trainer.py:793\u001b[0m, in \u001b[0;36mGradientDescentTrainer._try_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_epochs):\n\u001b[0;32m    792\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 793\u001b[0m     train_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(epoch)\n\u001b[0;32m    795\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs_completed \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_after_epochs_completed:\n\u001b[0;32m    796\u001b[0m         \u001b[39m# We're still catching up with the checkpoint, so we do nothing.\u001b[39;00m\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Note that we have to call _train_epoch() even when we know the epoch is skipped. We have to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[39m# time we train, even when starting from a checkpoint, so that we update the randomness\u001b[39;00m\n\u001b[0;32m    801\u001b[0m         \u001b[39m# generators in the same way each time.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs_completed \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\training\\gradient_descent_trainer.py:510\u001b[0m, in \u001b[0;36mGradientDescentTrainer._train_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_amp):\n\u001b[1;32m--> 510\u001b[0m     batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_outputs(batch, for_training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    511\u001b[0m     batch_group_outputs\u001b[39m.\u001b[39mappend(batch_outputs)\n\u001b[0;32m    512\u001b[0m     loss \u001b[39m=\u001b[39m batch_outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\training\\gradient_descent_trainer.py:403\u001b[0m, in \u001b[0;36mGradientDescentTrainer.batch_outputs\u001b[1;34m(self, batch, for_training)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_outputs\u001b[39m(\u001b[39mself\u001b[39m, batch: TensorDict, for_training: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    399\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[39m    Does a forward pass on the given batch and returns the output dictionary that the model\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39m    returns, after adding any specified regularization penalty to the loss (if training).\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     output_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pytorch_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m    405\u001b[0m     \u001b[39mif\u001b[39;00m for_training:\n\u001b[0;32m    406\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1573\u001b[0m     ):\n\u001b[0;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp_models\\coref\\models\\coref.py:180\u001b[0m, in \u001b[0;36mCoreferenceResolver.forward\u001b[1;34m(self, text, spans, span_labels, metadata)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39m# Parameters\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39m    A scalar loss to be optimised.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m# Shape: (batch_size, document_length, embedding_size)\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m text_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lexical_dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_text_field_embedder(text))\n\u001b[0;32m    182\u001b[0m batch_size \u001b[39m=\u001b[39m spans\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m    183\u001b[0m document_length \u001b[39m=\u001b[39m text_embeddings\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1573\u001b[0m     ):\n\u001b[0;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\modules\\text_field_embedders\\basic_text_field_embedder.py:102\u001b[0m, in \u001b[0;36mBasicTextFieldEmbedder.forward\u001b[1;34m(self, text_field_input, num_wrapping_dims, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m     token_vectors \u001b[39m=\u001b[39m embedder(\u001b[39mlist\u001b[39m(tensors\u001b[39m.\u001b[39mvalues())[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params_values)\n\u001b[0;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[39m# If there are multiple tensor arguments, we have to require matching names from the\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[39m# TokenIndexer.  I don't think there's an easy way around that.\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     token_vectors \u001b[39m=\u001b[39m embedder(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtensors, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params_values)\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m token_vectors \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[39m# To handle some very rare use cases, we allow the return value of the embedder to\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[39m# be None; we just skip it in that case.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     embedded_representations\u001b[39m.\u001b[39mappend(token_vectors)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1573\u001b[0m     ):\n\u001b[0;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: PretrainedTransformerMismatchedEmbedder.forward() got an unexpected keyword argument 'tokens'"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Define the AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=0.001) \n",
    "# Create the trainer and train \n",
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=validation_data_loader,\n",
    "    num_epochs=3,\n",
    "    cuda_device=0  # set to -1 to use CPU\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
