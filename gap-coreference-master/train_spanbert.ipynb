{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import DataLoader\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import PretrainedTransformerMismatchedEmbedder\n",
    "from allennlp.data import Instance\n",
    "import allennlp_models.coref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating config.jsonnet as plain json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'spanbert_local/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'spanbert_local/' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\gap-coreference-master\\train_spanbert.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Initialize the dataset reader from params\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dataset_reader_params \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mduplicate()\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mdataset_reader\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dataset_reader \u001b[39m=\u001b[39m DatasetReader\u001b[39m.\u001b[39;49mfrom_params(dataset_reader_params)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Read training and validation data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_data \u001b[39m=\u001b[39m dataset_reader\u001b[39m.\u001b[39mread(\u001b[39m\"\u001b[39m\u001b[39mgap-development.jsonl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\common\\from_params.py:604\u001b[0m, in \u001b[0;36mFromParams.from_params\u001b[1;34m(cls, params, constructor_to_call, constructor_to_inspect, **extras)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[39m# mypy can't follow the typing redirection that we do, so we explicitly cast here.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m     retyped_subclass \u001b[39m=\u001b[39m cast(Type[T], subclass)\n\u001b[1;32m--> 604\u001b[0m     \u001b[39mreturn\u001b[39;00m retyped_subclass\u001b[39m.\u001b[39mfrom_params(\n\u001b[0;32m    605\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    606\u001b[0m         constructor_to_call\u001b[39m=\u001b[39mconstructor_to_call,\n\u001b[0;32m    607\u001b[0m         constructor_to_inspect\u001b[39m=\u001b[39mconstructor_to_inspect,\n\u001b[0;32m    608\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextras,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     \u001b[39m# In some rare cases, we get a registered subclass that does _not_ have a\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     \u001b[39m# from_params method (this happens with Activations, for instance, where we\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    615\u001b[0m     \u001b[39m# you've done the right thing in passing your parameters, and nothing else needs to\u001b[39;00m\n\u001b[0;32m    616\u001b[0m     \u001b[39m# be recursively constructed.\u001b[39;00m\n\u001b[0;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m subclass(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\common\\from_params.py:636\u001b[0m, in \u001b[0;36mFromParams.from_params\u001b[1;34m(cls, params, constructor_to_call, constructor_to_inspect, **extras)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m     \u001b[39m# This class has a constructor, so create kwargs for it.\u001b[39;00m\n\u001b[0;32m    635\u001b[0m     constructor_to_inspect \u001b[39m=\u001b[39m cast(Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, T], constructor_to_inspect)\n\u001b[1;32m--> 636\u001b[0m     kwargs \u001b[39m=\u001b[39m create_kwargs(constructor_to_inspect, \u001b[39mcls\u001b[39m, params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextras)\n\u001b[0;32m    638\u001b[0m \u001b[39mreturn\u001b[39;00m constructor_to_call(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\common\\from_params.py:206\u001b[0m, in \u001b[0;36mcreate_kwargs\u001b[1;34m(constructor, cls, params, **extras)\u001b[0m\n\u001b[0;32m    203\u001b[0m annotation \u001b[39m=\u001b[39m remove_optional(param\u001b[39m.\u001b[39mannotation)\n\u001b[0;32m    205\u001b[0m explicitly_set \u001b[39m=\u001b[39m param_name \u001b[39min\u001b[39;00m params\n\u001b[1;32m--> 206\u001b[0m constructed_arg \u001b[39m=\u001b[39m pop_and_construct_arg(\n\u001b[0;32m    207\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, param_name, annotation, param\u001b[39m.\u001b[39mdefault, params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextras\n\u001b[0;32m    208\u001b[0m )\n\u001b[0;32m    210\u001b[0m \u001b[39m# If the param wasn't explicitly set in `params` and we just ended up constructing\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39m# the default value for the parameter, we can just omit it.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m# Leaving it in can cause issues with **kwargs in some corner cases, where you might end up\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m# with multiple values for a single parameter (e.g., the default value gives you lazy=False\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[39m# for a dataset reader inside **kwargs, but a particular dataset reader actually hard-codes\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[39m# lazy=True - the superclass sees both lazy=True and lazy=False in its constructor).\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[39mif\u001b[39;00m explicitly_set \u001b[39mor\u001b[39;00m constructed_arg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m param\u001b[39m.\u001b[39mdefault:\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\common\\from_params.py:314\u001b[0m, in \u001b[0;36mpop_and_construct_arg\u001b[1;34m(class_name, argument_name, annotation, default, params, **extras)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m popped_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m \u001b[39mreturn\u001b[39;00m construct_arg(class_name, name, popped_params, annotation, default, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextras)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\common\\from_params.py:394\u001b[0m, in \u001b[0;36mconstruct_arg\u001b[1;34m(class_name, argument_name, popped_params, annotation, default, **extras)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m{\u001b[39;00margument_name\u001b[39m}\u001b[39;00m\u001b[39m to be a Mapping (probably a dict or a Params object).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         )\n\u001b[0;32m    393\u001b[0m     \u001b[39mfor\u001b[39;00m key, value_params \u001b[39min\u001b[39;00m popped_params\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 394\u001b[0m         value_dict[key] \u001b[39m=\u001b[39m construct_arg(\n\u001b[0;32m    395\u001b[0m             \u001b[39mstr\u001b[39m(value_cls),\n\u001b[0;32m    396\u001b[0m             argument_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m key,\n\u001b[0;32m    397\u001b[0m             value_params,\n\u001b[0;32m    398\u001b[0m             value_cls,\n\u001b[0;32m    399\u001b[0m             _NO_DEFAULT,\n\u001b[0;32m    400\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextras,\n\u001b[0;32m    401\u001b[0m         )\n\u001b[0;32m    403\u001b[0m     \u001b[39mreturn\u001b[39;00m value_dict\n\u001b[0;32m    405\u001b[0m \u001b[39melif\u001b[39;00m origin \u001b[39min\u001b[39;00m (Tuple, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(can_construct_from_params(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args):\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\common\\from_params.py:348\u001b[0m, in \u001b[0;36mconstruct_arg\u001b[1;34m(class_name, argument_name, popped_params, annotation, default, **extras)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(popped_params, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    347\u001b[0m         popped_params \u001b[39m=\u001b[39m Params(popped_params)\n\u001b[1;32m--> 348\u001b[0m     result \u001b[39m=\u001b[39m annotation\u001b[39m.\u001b[39mfrom_params(params\u001b[39m=\u001b[39mpopped_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msubextras)\n\u001b[0;32m    350\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    351\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m optional:\n\u001b[0;32m    352\u001b[0m     \u001b[39m# Not optional and not supplied, that's an error!\u001b[39;00m\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\common\\from_params.py:604\u001b[0m, in \u001b[0;36mFromParams.from_params\u001b[1;34m(cls, params, constructor_to_call, constructor_to_inspect, **extras)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[39m# mypy can't follow the typing redirection that we do, so we explicitly cast here.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m     retyped_subclass \u001b[39m=\u001b[39m cast(Type[T], subclass)\n\u001b[1;32m--> 604\u001b[0m     \u001b[39mreturn\u001b[39;00m retyped_subclass\u001b[39m.\u001b[39mfrom_params(\n\u001b[0;32m    605\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    606\u001b[0m         constructor_to_call\u001b[39m=\u001b[39mconstructor_to_call,\n\u001b[0;32m    607\u001b[0m         constructor_to_inspect\u001b[39m=\u001b[39mconstructor_to_inspect,\n\u001b[0;32m    608\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextras,\n\u001b[0;32m    609\u001b[0m     )\n\u001b[0;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    611\u001b[0m     \u001b[39m# In some rare cases, we get a registered subclass that does _not_ have a\u001b[39;00m\n\u001b[0;32m    612\u001b[0m     \u001b[39m# from_params method (this happens with Activations, for instance, where we\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    615\u001b[0m     \u001b[39m# you've done the right thing in passing your parameters, and nothing else needs to\u001b[39;00m\n\u001b[0;32m    616\u001b[0m     \u001b[39m# be recursively constructed.\u001b[39;00m\n\u001b[0;32m    617\u001b[0m     \u001b[39mreturn\u001b[39;00m subclass(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\common\\from_params.py:638\u001b[0m, in \u001b[0;36mFromParams.from_params\u001b[1;34m(cls, params, constructor_to_call, constructor_to_inspect, **extras)\u001b[0m\n\u001b[0;32m    635\u001b[0m     constructor_to_inspect \u001b[39m=\u001b[39m cast(Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, T], constructor_to_inspect)\n\u001b[0;32m    636\u001b[0m     kwargs \u001b[39m=\u001b[39m create_kwargs(constructor_to_inspect, \u001b[39mcls\u001b[39m, params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextras)\n\u001b[1;32m--> 638\u001b[0m \u001b[39mreturn\u001b[39;00m constructor_to_call(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\data\\token_indexers\\pretrained_transformer_mismatched_indexer.py:58\u001b[0m, in \u001b[0;36mPretrainedTransformerMismatchedIndexer.__init__\u001b[1;34m(self, model_name, namespace, max_length, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     57\u001b[0m \u001b[39m# The matched version v.s. mismatched\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_matched_indexer \u001b[39m=\u001b[39m PretrainedTransformerIndexer(\n\u001b[0;32m     59\u001b[0m     model_name,\n\u001b[0;32m     60\u001b[0m     namespace\u001b[39m=\u001b[39mnamespace,\n\u001b[0;32m     61\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m     62\u001b[0m     tokenizer_kwargs\u001b[39m=\u001b[39mtokenizer_kwargs,\n\u001b[0;32m     63\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allennlp_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_matched_indexer\u001b[39m.\u001b[39m_allennlp_tokenizer\n\u001b[0;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_matched_indexer\u001b[39m.\u001b[39m_tokenizer\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\data\\token_indexers\\pretrained_transformer_indexer.py:56\u001b[0m, in \u001b[0;36mPretrainedTransformerIndexer.__init__\u001b[1;34m(self, model_name, namespace, max_length, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_namespace \u001b[39m=\u001b[39m namespace\n\u001b[1;32m---> 56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allennlp_tokenizer \u001b[39m=\u001b[39m PretrainedTransformerTokenizer(\n\u001b[0;32m     57\u001b[0m     model_name, tokenizer_kwargs\u001b[39m=\u001b[39;49mtokenizer_kwargs\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allennlp_tokenizer\u001b[39m.\u001b[39mtokenizer\n\u001b[0;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_added_to_vocabulary \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\data\\tokenizers\\pretrained_transformer_tokenizer.py:72\u001b[0m, in \u001b[0;36mPretrainedTransformerTokenizer.__init__\u001b[1;34m(self, model_name, add_special_tokens, max_length, tokenizer_kwargs, verification_tokens)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_name \u001b[39m=\u001b[39m model_name\n\u001b[0;32m     70\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mallennlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m cached_transformers\n\u001b[1;32m---> 72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m cached_transformers\u001b[39m.\u001b[39mget_tokenizer(\n\u001b[0;32m     73\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_name, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer_kwargs\n\u001b[0;32m     74\u001b[0m )\n\u001b[0;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_special_tokens \u001b[39m=\u001b[39m add_special_tokens\n\u001b[0;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_length \u001b[39m=\u001b[39m max_length\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\common\\cached_transformers.py:204\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[1;34m(model_name, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m tokenizer \u001b[39m=\u001b[39m _tokenizer_cache\u001b[39m.\u001b[39mget(cache_key, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    203\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     tokenizer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mAutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    205\u001b[0m         model_name,\n\u001b[0;32m    206\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    207\u001b[0m     )\n\u001b[0;32m    208\u001b[0m     _tokenizer_cache[cache_key] \u001b[39m=\u001b[39m tokenizer\n\u001b[0;32m    209\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:597\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[0;32m    596\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    598\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    599\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1767\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1761\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   1762\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1763\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1764\u001b[0m     )\n\u001b[0;32m   1766\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m-> 1767\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1768\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1769\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1770\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1771\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1772\u001b[0m     )\n\u001b[0;32m   1774\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1775\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'spanbert_local/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'spanbert_local/' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "# Read the config file and create a Params object\n",
    "params = Params.from_file(\"config.jsonnet\")\n",
    "# Initialize the dataset reader from params\n",
    "dataset_reader_params = params.duplicate().pop(\"dataset_reader\")\n",
    "dataset_reader = DatasetReader.from_params(dataset_reader_params)\n",
    "\n",
    "# Read training and validation data\n",
    "train_data = dataset_reader.read(\"gap-development.jsonl\")\n",
    "validation_data = dataset_reader.read(\"gap-validation.jsonl\")\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = Vocabulary.from_instances(train_data + validation_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update the instances with the vocabulary\n",
    "for instance in train_data + validation_data:\n",
    "    instance.index_fields(vocab)\n",
    "\n",
    "# Initialize data loaders\n",
    "train_data_loader = DataLoader(train_data, batch_sampler=BucketBatchSampler(train_data, batch_size=4, sorting_keys=[\"tokens\"]))\n",
    "validation_data_loader = DataLoader(validation_data, batch_size=4)\n",
    "\n",
    "# Initialize the model from params\n",
    "model_params = params.pop(\"model\")\n",
    "model = Model.from_params(vocab=vocab, params=model_params)\n",
    "\n",
    "# Initialize the trainer from params and train the model\n",
    "trainer_params = params.pop(\"trainer\")\n",
    "trainer = GradientDescentTrainer.from_params(\n",
    "    model=model,\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=validation_data_loader,\n",
    "    params=trainer_params\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'fine_tune' from 'allennlp.training' (f:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\training\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\gap-coreference-master\\train_spanbert.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mallennlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marchival\u001b[39;00m \u001b[39mimport\u001b[39;00m load_archive\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mallennlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m fine_tune\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Load the pre-trained archive (replace with the path to your downloaded archive)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m archive \u001b[39m=\u001b[39m load_archive(\u001b[39m\"\u001b[39m\u001b[39mspanbert_local/coref-spanbert-large-2021.03.10.tar.gz\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'fine_tune' from 'allennlp.training' (f:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\training\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from allennlp.models.archival import load_archive\n",
    "from allennlp.training import fine_tune\n",
    "\n",
    "# Load the pre-trained archive (replace with the path to your downloaded archive)\n",
    "archive = load_archive(\"spanbert_local/coref-spanbert-large-2021.03.10.tar.gz\")\n",
    "\n",
    "# Fine-tune the model on your new dataset\n",
    "fine_tune_model = fine_tune.fine_tune_model_from_archive(\n",
    "    archive,\n",
    "    'gap-development.jsonl',  # Replace with the path to your training data\n",
    "    'gap-validation.jsonl',  # Replace with the path to your validation data\n",
    "    from_params=None,\n",
    "    extend_vocab=False,  # set to True if your new data has a different vocabulary\n",
    "    cuda_device=0  # set to -1 to use CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\pc-bae-2\\AppData\\Local\\Temp\\tmpqorou873\\config.json as plain json\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data import DataLoader\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.params import Params\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import PretrainedTransformerMismatchedEmbedder\n",
    "from allennlp.data import Instance\n",
    "from allennlp.models.archival import load_archive\n",
    "\n",
    "# Load the pre-trained archive\n",
    "archive = load_archive(\"spanbert_local/coref-spanbert-large-2021.03.10.tar.gz\")\n",
    "model = archive.model\n",
    "# Make sure to move the model to the correct device\n",
    "model = model.cuda(0)  # Replace with model.cpu() if you don't want to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_reader.Params({'type': 'coref', 'max_sentences': 110, 'max_span_width': 30, 'token_indexers': {'tokens': {'type': 'pretrained_transformer_mismatched', 'max_length': 512, 'model_name': 'SpanBERT/spanbert-large-cased'}}})\n"
     ]
    }
   ],
   "source": [
    "print(archive.config[\"dataset_reader\"])\n",
    "\n",
    "train_path = \"gap-development.jsonl\"  \n",
    "val_path = \"gap-validation.jsonl\" \n",
    "# Load your training and validation data\n",
    "reader = DatasetReader.from_params(archive.config[\"dataset_reader\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gap-validation.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07e2b6430a6411db194e52d247d0d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6597592cf045b49b1d619a9d0e9550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2887ceec7f84ef78bdb8ec793e505fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary with namespaces:\n",
      " \tNon Padded Namespaces: {'*tags', '*labels'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # You can extend the vocabulary here if needed\n",
    "# vocab = model.vocab\n",
    "# vocab.extend_from_instances(train_data)\n",
    "# vocab.extend_from_instances(validation_data)\n",
    "\n",
    "# Create data loaders\n",
    "from allennlp.data.data_loaders import MultiProcessDataLoader \n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "from allennlp.data import Vocabulary\n",
    "\n",
    "\n",
    "\n",
    "# Make sure your model also uses the same vocabulary\n",
    "model.vocab = vocab\n",
    "\n",
    "train_sampler = BucketBatchSampler(batch_size=4, sorting_keys=[\"tokens\"])\n",
    "print(val_path)\n",
    "train_data_loader = MultiProcessDataLoader(reader=reader,data_path=train_path, batch_sampler=train_sampler)\n",
    "validation_data_loader = MultiProcessDataLoader(reader=reader,data_path=val_path, batch_size=4)\n",
    "\n",
    "# Build a vocabulary from the training data\n",
    "vocab = Vocabulary.from_instances(train_data)\n",
    "# Index the data loaders with the vocabulary\n",
    "train_data_loader.index_with(vocab)\n",
    "validation_data_loader.index_with(vocab)\n",
    "\n",
    "# Make sure your model also uses the same vocabulary\n",
    "model.vocab = vocab\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import DatasetReader, Instance, Field\n",
    "from allennlp.data.fields import TextField, SpanField, ListField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer, SpacyTokenizer\n",
    "from typing import Iterator, List, Dict, Any\n",
    "import json\n",
    "\n",
    "class CustomCorefReader(DatasetReader):\n",
    "    def __init__(self, \n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 **kwargs: Any):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer = tokenizer or SpacyTokenizer()\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                example = json.loads(line)\n",
    "                text = example['text']\n",
    "                clusters = example['clusters']\n",
    "\n",
    "                tokens_with_offsets = self.tokenizer.tokenize(text)\n",
    "                tokens = [token.text for token in tokens_with_offsets]\n",
    "                token_offsets = [(token.idx, token.idx + len(token.text)) for token in tokens_with_offsets]\n",
    "\n",
    "                text_field = TextField(tokens_with_offsets, self.token_indexers)\n",
    "                \n",
    "                # Convert character-based spans to token-based spans\n",
    "                token_based_clusters = []\n",
    "                for cluster in clusters:\n",
    "                    token_based_cluster = []\n",
    "                    for char_start, char_end in cluster:\n",
    "                        try:\n",
    "                            token_start = next(i for i, (t_start, t_end) in enumerate(token_offsets) if t_start <= char_start < t_end)\n",
    "                        except StopIteration:\n",
    "                            print(f\"StopIteration for token_start: char_start={char_start}, token_offsets={token_offsets}\")\n",
    "                            raise\n",
    "\n",
    "                        try:\n",
    "                            token_end = next(i for i, (t_start, t_end) in enumerate(token_offsets) if t_start < char_end <= t_end)\n",
    "                        except StopIteration as e:\n",
    "                            print(f\"StopIteration for token_end: char_end={char_end}, token_offsets={token_offsets}\")\n",
    "                            # Assuming `text` contains the actual text content.\n",
    "                            print(f\"Text around char_end: {text[char_end-10:char_end+10]}\")\n",
    "\n",
    "                            continue\n",
    "\n",
    "                        token_based_cluster.append((token_start, token_end))\n",
    "                    token_based_clusters.append(token_based_cluster)\n",
    "\n",
    "                span_fields: List[Field] = []\n",
    "                for token_based_cluster in token_based_clusters:\n",
    "                    for start, end in token_based_cluster:\n",
    "                        span_fields.append(SpanField(start, end, text_field))\n",
    "\n",
    "                span_list_field = ListField(span_fields)\n",
    "            \n",
    "                fields: Dict[str, Field] = {'text': text_field, 'spans': span_list_field}\n",
    "                yield Instance(fields)\n",
    "\n",
    "reader = CustomCorefReader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopIteration for token_end: char_end=323, token_offsets=[(0, 2), (3, 12), (13, 15), (16, 24), (25, 31), (32, 34), (35, 42), (43, 48), (48, 49), (50, 51), (51, 52), (52, 55), (56, 61), (62, 67), (68, 70), (71, 73), (74, 77), (78, 85), (86, 92), (93, 97), (98, 104), (105, 107), (108, 114), (114, 116), (117, 127), (128, 134), (135, 143), (144, 146), (147, 150), (151, 157), (158, 166), (167, 169), (170, 179), (180, 188), (189, 191), (192, 195), (196, 201), (202, 204), (205, 206), (207, 211), (212, 216), (216, 217), (218, 224), (225, 235), (236, 243), (244, 251), (252, 254), (255, 263), (264, 269), (269, 270), (271, 279), (280, 283), (284, 295), (295, 296), (297, 301), (302, 304), (305, 308), (309, 316), (316, 317), (318, 322), (323, 324), (325, 329), (330, 333), (334, 343), (344, 347), (348, 361), (362, 366), (367, 370), (371, 380), (381, 385), (386, 388), (389, 400), (401, 407), (407, 409), (410, 414), (415, 417), (418, 420), (421, 426), (427, 430), (431, 437), (438, 440), (441, 444), (445, 450), (450, 451), (451, 453)]\n",
      "Text around char_end: ery, Jack * fans are\n",
      "StopIteration for token_end: char_end=285, token_offsets=[(0, 8), (9, 12), (13, 14), (15, 28), (29, 35), (36, 38), (39, 44), (45, 51), (52, 55), (56, 58), (59, 63), (64, 69), (70, 73), (74, 77), (78, 82), (83, 85), (86, 89), (90, 93), (94, 101), (101, 102), (103, 106), (107, 111), (112, 116), (116, 118), (119, 124), (125, 133), (134, 141), (141, 142), (143, 145), (146, 153), (154, 160), (161, 163), (164, 173), (174, 176), (177, 180), (181, 185), (185, 186), (187, 192), (193, 202), (203, 207), (208, 210), (211, 216), (216, 217), (218, 225), (226, 235), (235, 236), (237, 239), (240, 243), (244, 247), (248, 257), (258, 260), (261, 266), (266, 267), (268, 273), (274, 279), (280, 284), (285, 287), (288, 295), (296, 298), (299, 304), (305, 313), (313, 314), (315, 319), (320, 323), (324, 335), (336, 338), (339, 347), (348, 350), (351, 357), (358, 364), (364, 365), (366, 379), (380, 382), (383, 390), (391, 394), (395, 397), (398, 404), (404, 405), (406, 409), (410, 420), (421, 423), (424, 429), (429, 430), (431, 434), (435, 438), (439, 448), (449, 453), (454, 457), (458, 463), (464, 466), (467, 468), (469, 474), (475, 480), (480, 481), (482, 485), (486, 491), (492, 498), (499, 502), (503, 509), (510, 513), (514, 523), (524, 534), (535, 537), (538, 539), (540, 548), (549, 554), (554, 555), (556, 563), (564, 570), (571, 574), (575, 584), (585, 595), (595, 596)]\n",
      "Text around char_end: amed John C. Calhoun\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855bb766153241688ec0da40ba700e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f23763a047e497dbf65c21e73abc87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopIteration for token_end: char_end=323, token_offsets=[(0, 2), (3, 12), (13, 15), (16, 24), (25, 31), (32, 34), (35, 42), (43, 48), (48, 49), (50, 51), (51, 52), (52, 55), (56, 61), (62, 67), (68, 70), (71, 73), (74, 77), (78, 85), (86, 92), (93, 97), (98, 104), (105, 107), (108, 114), (114, 116), (117, 127), (128, 134), (135, 143), (144, 146), (147, 150), (151, 157), (158, 166), (167, 169), (170, 179), (180, 188), (189, 191), (192, 195), (196, 201), (202, 204), (205, 206), (207, 211), (212, 216), (216, 217), (218, 224), (225, 235), (236, 243), (244, 251), (252, 254), (255, 263), (264, 269), (269, 270), (271, 279), (280, 283), (284, 295), (295, 296), (297, 301), (302, 304), (305, 308), (309, 316), (316, 317), (318, 322), (323, 324), (325, 329), (330, 333), (334, 343), (344, 347), (348, 361), (362, 366), (367, 370), (371, 380), (381, 385), (386, 388), (389, 400), (401, 407), (407, 409), (410, 414), (415, 417), (418, 420), (421, 426), (427, 430), (431, 437), (438, 440), (441, 444), (445, 450), (450, 451), (451, 453)]\n",
      "Text around char_end: ery, Jack * fans are\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e4590352fa4f33b254c7928d258509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StopIteration for token_end: char_end=285, token_offsets=[(0, 8), (9, 12), (13, 14), (15, 28), (29, 35), (36, 38), (39, 44), (45, 51), (52, 55), (56, 58), (59, 63), (64, 69), (70, 73), (74, 77), (78, 82), (83, 85), (86, 89), (90, 93), (94, 101), (101, 102), (103, 106), (107, 111), (112, 116), (116, 118), (119, 124), (125, 133), (134, 141), (141, 142), (143, 145), (146, 153), (154, 160), (161, 163), (164, 173), (174, 176), (177, 180), (181, 185), (185, 186), (187, 192), (193, 202), (203, 207), (208, 210), (211, 216), (216, 217), (218, 225), (226, 235), (235, 236), (237, 239), (240, 243), (244, 247), (248, 257), (258, 260), (261, 266), (266, 267), (268, 273), (274, 279), (280, 284), (285, 287), (288, 295), (296, 298), (299, 304), (305, 313), (313, 314), (315, 319), (320, 323), (324, 335), (336, 338), (339, 347), (348, 350), (351, 357), (358, 364), (364, 365), (366, 379), (380, 382), (383, 390), (391, 394), (395, 397), (398, 404), (404, 405), (406, 409), (410, 420), (421, 423), (424, 429), (429, 430), (431, 434), (435, 438), (439, 448), (449, 453), (454, 457), (458, 463), (464, 466), (467, 468), (469, 474), (475, 480), (480, 481), (482, 485), (486, 491), (492, 498), (499, 502), (503, 509), (510, 513), (514, 523), (524, 534), (535, 537), (538, 539), (540, 548), (549, 554), (554, 555), (556, 563), (564, 570), (571, 574), (575, 584), (585, 595), (595, 596)]\n",
      "Text around char_end: amed John C. Calhoun\n",
      "Number of instances in train_data: 2000\n",
      "Number of instances in validation_data: 454\n"
     ]
    }
   ],
   "source": [
    "# Read the training and validation data\n",
    "train_data = list(reader.read(train_path))\n",
    "validation_data = list(reader.read(val_path))\n",
    "print(len(train_data))\n",
    "# Build a vocabulary from the training data\n",
    "vocab = Vocabulary.from_instances(train_data)\n",
    "\n",
    "# Create data loaders\n",
    "from allennlp.data.data_loaders import MultiProcessDataLoader\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "\n",
    "train_sampler = BucketBatchSampler(batch_size=4, sorting_keys=[\"text\"])\n",
    "\n",
    "train_data_loader = MultiProcessDataLoader(reader=reader, data_path=train_path, batch_sampler=train_sampler)\n",
    "validation_data_loader = MultiProcessDataLoader(reader=reader, data_path=val_path, batch_size=4)\n",
    "\n",
    "# Index the data loaders with the vocabulary\n",
    "train_data_loader.index_with(vocab)\n",
    "validation_data_loader.index_with(vocab)\n",
    "\n",
    "# Make sure your model also uses the same vocabulary\n",
    "model.vocab = vocab\n",
    "\n",
    "# Debugging: Print the number of instances in the data\n",
    "print(\"Number of instances in train_data:\", len(train_data))\n",
    "print(\"Number of instances in validation_data:\", len(validation_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57cb68c7a6084d68a01e02e73b25ee07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "PretrainedTransformerMismatchedEmbedder.forward() got an unexpected keyword argument 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\gap-coreference-master\\train_spanbert.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Create the trainer and train\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m trainer \u001b[39m=\u001b[39m GradientDescentTrainer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     cuda_device\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m  \u001b[39m# set to -1 to use CPU\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/UvA%202023/DL%204%20NLP/Project/DLNLP-pronoun-resolution/DLNLP-pronoun-resolution/gap-coreference-master/train_spanbert.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\training\\gradient_descent_trainer.py:771\u001b[0m, in \u001b[0;36mGradientDescentTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    768\u001b[0m metrics \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 771\u001b[0m     metrics, epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_train()\n\u001b[0;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m metrics\n\u001b[0;32m    773\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\training\\gradient_descent_trainer.py:793\u001b[0m, in \u001b[0;36mGradientDescentTrainer._try_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_epochs):\n\u001b[0;32m    792\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> 793\u001b[0m     train_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(epoch)\n\u001b[0;32m    795\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs_completed \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_after_epochs_completed:\n\u001b[0;32m    796\u001b[0m         \u001b[39m# We're still catching up with the checkpoint, so we do nothing.\u001b[39;00m\n\u001b[0;32m    797\u001b[0m         \u001b[39m# Note that we have to call _train_epoch() even when we know the epoch is skipped. We have to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[39m# time we train, even when starting from a checkpoint, so that we update the randomness\u001b[39;00m\n\u001b[0;32m    801\u001b[0m         \u001b[39m# generators in the same way each time.\u001b[39;00m\n\u001b[0;32m    802\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs_completed \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\training\\gradient_descent_trainer.py:510\u001b[0m, in \u001b[0;36mGradientDescentTrainer._train_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_amp):\n\u001b[1;32m--> 510\u001b[0m     batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_outputs(batch, for_training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    511\u001b[0m     batch_group_outputs\u001b[39m.\u001b[39mappend(batch_outputs)\n\u001b[0;32m    512\u001b[0m     loss \u001b[39m=\u001b[39m batch_outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\training\\gradient_descent_trainer.py:403\u001b[0m, in \u001b[0;36mGradientDescentTrainer.batch_outputs\u001b[1;34m(self, batch, for_training)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_outputs\u001b[39m(\u001b[39mself\u001b[39m, batch: TensorDict, for_training: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    399\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[39m    Does a forward pass on the given batch and returns the output dictionary that the model\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39m    returns, after adding any specified regularization penalty to the loss (if training).\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     output_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pytorch_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m    405\u001b[0m     \u001b[39mif\u001b[39;00m for_training:\n\u001b[0;32m    406\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1573\u001b[0m     ):\n\u001b[0;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp_models\\coref\\models\\coref.py:180\u001b[0m, in \u001b[0;36mCoreferenceResolver.forward\u001b[1;34m(self, text, spans, span_labels, metadata)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39m# Parameters\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39m    A scalar loss to be optimised.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m# Shape: (batch_size, document_length, embedding_size)\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m text_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lexical_dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_text_field_embedder(text))\n\u001b[0;32m    182\u001b[0m batch_size \u001b[39m=\u001b[39m spans\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m    183\u001b[0m document_length \u001b[39m=\u001b[39m text_embeddings\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1573\u001b[0m     ):\n\u001b[0;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\allennlp\\modules\\text_field_embedders\\basic_text_field_embedder.py:102\u001b[0m, in \u001b[0;36mBasicTextFieldEmbedder.forward\u001b[1;34m(self, text_field_input, num_wrapping_dims, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m     token_vectors \u001b[39m=\u001b[39m embedder(\u001b[39mlist\u001b[39m(tensors\u001b[39m.\u001b[39mvalues())[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params_values)\n\u001b[0;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[39m# If there are multiple tensor arguments, we have to require matching names from the\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[39m# TokenIndexer.  I don't think there's an easy way around that.\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     token_vectors \u001b[39m=\u001b[39m embedder(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtensors, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params_values)\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m token_vectors \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[39m# To handle some very rare use cases, we allow the return value of the embedder to\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[39m# be None; we just skip it in that case.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     embedded_representations\u001b[39m.\u001b[39mappend(token_vectors)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1565\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1566\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1568\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1569\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1570\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[0;32m   1571\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1572\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[0;32m   1573\u001b[0m     ):\n\u001b[0;32m   1574\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: PretrainedTransformerMismatchedEmbedder.forward() got an unexpected keyword argument 'tokens'"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Define the AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=0.001) \n",
    "# Create the trainer and train \n",
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=validation_data_loader,\n",
    "    num_epochs=3,\n",
    "    cuda_device=0  # set to -1 to use CPU\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
