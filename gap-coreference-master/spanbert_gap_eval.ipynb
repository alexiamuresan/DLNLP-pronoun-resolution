{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available, predictions will be faster.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from gap_scorer import run_scorer  \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# Check for CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available, predictions will be faster.\")\n",
    "else:\n",
    "    print(\"CUDA not available, predictions may be slower.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\pc-bae-2\\AppData\\Local\\Temp\\tmp5catso87\\config.json as plain json\n",
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "f:\\UvA 2023\\DL 4 NLP\\Project\\DLNLP-pronoun-resolution\\DLNLP-pronoun-resolution\\spanbert_env\\lib\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SpanBERT predictor\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Path to the file\n",
    "path_spanbert = 'spanbert_local/'\n",
    "filename = \"coref-spanbert-large-2021.03.10.tar.gz\"\n",
    "\n",
    "\n",
    "save_path = os.path.join(path_spanbert, filename)\n",
    "if os.path.exists(save_path):\n",
    "    predictor = Predictor.from_path(save_path,cuda_device=0)\n",
    "else:\n",
    "        # Create directory if it doesn't exist\n",
    "    if not os.path.exists(path_spanbert):\n",
    "        os.makedirs(path_spanbert)\n",
    "        # Full path to save the file\n",
    "\n",
    "    # Download the file\n",
    "    urllib.request.urlretrieve(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\", save_path)\n",
    "    predictor = Predictor.from_path(save_path,cuda_device=0)\n",
    "\n",
    "    # predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\",cuda_device=0) #use this if urllib didnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ID                                               Text Pronoun  \\\n",
      "0  validation-1  He admitted making four trips to China and pla...     him   \n",
      "1  validation-2  Kathleen Nott was born in Camberwell, London. ...     She   \n",
      "2  validation-3  When she returns to her hotel room, a Liberian...     his   \n",
      "3  validation-4  On 19 March 2007, during a campaign appearance...      he   \n",
      "4  validation-5  By this time, Karen Blixen had separated from ...     she   \n",
      "\n",
      "   Pronoun-offset                   A  A-offset  A-coref              B  \\\n",
      "0             256  Jose de Venecia Jr       208    False         Abalos   \n",
      "1             185               Ellen       110    False       Kathleen   \n",
      "2             435     Jason Scott Lee       383    False          Danny   \n",
      "3             333           Reucassel       300     True         Debnam   \n",
      "4             427        Finch Hatton       290    False  Beryl Markham   \n",
      "\n",
      "   B-offset  B-coref                                                URL  \n",
      "0       241    False  http://en.wikipedia.org/wiki/Commission_on_Ele...  \n",
      "1       150     True         http://en.wikipedia.org/wiki/Kathleen_Nott  \n",
      "2       406     True  http://en.wikipedia.org/wiki/Hawaii_Five-0_(20...  \n",
      "3       325    False       http://en.wikipedia.org/wiki/Craig_Reucassel  \n",
      "4       328     True    http://en.wikipedia.org/wiki/Denys_Finch_Hatton  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read GAP data\n",
    "main_path = \"../gap-coreference-master/gap-coreference-master\"  \n",
    "main_path = \".\" \n",
    "gap_file =  \"gap-test.tsv\"\n",
    "gap_development_path = os.path.join(main_path,gap_file)\n",
    "gap_df = pd.read_csv(gap_development_path, delimiter='\\t')\n",
    "print(gap_df[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                                     validation-3\n",
      "Text              When she returns to her hotel room, a Liberian...\n",
      "Pronoun                                                         his\n",
      "Pronoun-offset                                                  435\n",
      "A                                                   Jason Scott Lee\n",
      "A-offset                                                        383\n",
      "A-coref                                                       False\n",
      "B                                                             Danny\n",
      "B-offset                                                        406\n",
      "B-coref                                                        True\n",
      "URL               http://en.wikipedia.org/wiki/Hawaii_Five-0_(20...\n",
      "Name: 2, dtype: object\n",
      "[[[1, 1], [4, 4], [16, 16], [41, 41], [45, 45]], [[48, 49], [59, 60]], [[32, 37], [65, 65]], [[19, 25], [68, 70]], [[85, 85], [90, 90]], [[52, 53], [95, 95]]]\n",
      "When she returns to her hotel room, a Liberian man (Tony Todd) forces her to smuggle $20 million worth of conflict diamonds to New York, or else fellow fight attendant and friend Angela will die. She is caught before she can board the flight, and the team now have nine hours until the plane lands, and save Angela. After the confiscated diamonds are stolen by the brother of Kaleo (Jason Scott Lee), whom Danny put away for murdering his partner last year, Five-0 and Chief Fryer team up and enlist the help of August March (Ed Asner), who served a 30-year sentence for smuggling diamonds.\n",
      "When [she] returns to [her] hotel room , a Liberian man ( Tony Todd ) forces [her] to smuggle [$ 20 million worth of conflict diamonds] to New York , or else [fellow fight attendant and friend Angela] will die . [She] is caught before [she] can board [the flight] , and [the team] now have nine hours until [the plane] lands , and save [Angela] . After [the confiscated diamonds] are stolen by the brother of Kaleo ( Jason Scott Lee ) , whom [Danny] put away for murdering [his] partner last year , [Five-0] and Chief Fryer team up and enlist the help of August March ( Ed Asner ) , who served a 30 - year sentence for smuggling diamonds .\n"
     ]
    }
   ],
   "source": [
    "# Sample output from AllenNLP coref model\n",
    "example = gap_df.iloc[2]\n",
    "text = example['Text']\n",
    "print(example)\n",
    "result = predictor.predict(document=text)\n",
    "print(result['clusters'])\n",
    "# Initialize an empty list with placeholders\n",
    "text_visual = ['_'] * len(result['document'])\n",
    "\n",
    "# Fill in the placeholders with tokens\n",
    "for i, token in enumerate(result['document']):\n",
    "    text_visual[i] = token\n",
    "\n",
    "# Add brackets for coref clusters\n",
    "for cluster in result['clusters']:\n",
    "    for start, end in cluster:\n",
    "        text_visual[start] = '[' + text_visual[start]\n",
    "        text_visual[end] = text_visual[end] + ']'\n",
    "\n",
    "# Combine into a string\n",
    "text_visual_str = ' '.join(text_visual)\n",
    "print(text)\n",
    "print(text_visual_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 1], [4, 4], [16, 16], [41, 41], [45, 45]], [[48, 49], [59, 60]], [[32, 37], [65, 65]], [[19, 25], [68, 70]], [[85, 85], [90, 90]], [[52, 53], [95, 95]]]\n",
      "[['she', 'her', 'her', 'She', 'she'], ['the flight', 'the plane'], ['fellow fight attendant and friend Angela', 'Angela'], ['$ 20 million worth of conflict diamonds', 'the confiscated diamonds'], ['Danny', 'his'], ['the team', 'Five-0']]\n"
     ]
    }
   ],
   "source": [
    "# Define the function to extract words that correspond to each cluster\n",
    "def extract_cluster_words(tokenized_document, clusters):\n",
    "    total_clusters = []\n",
    "    for cluster in clusters:\n",
    "        txt_cluster = []\n",
    "        for start, end in cluster:\n",
    "            cluster_tokens = tokenized_document[start:end+1]\n",
    "            txt_cluster.append(\" \".join(cluster_tokens))\n",
    "        total_clusters.append(txt_cluster)\n",
    "    return total_clusters\n",
    "\n",
    "# Example \n",
    "tokenized_document = ['It', 'was', 'reported', 'that', 'John', 'and', 'Jane', 'were', 'together', '.', 'He', 'said', 'it', 'was', 'true', '.']\n",
    "clusters = [[[4, 4], [10, 10]], [[5, 5], [12, 12]]]\n",
    "print(result['clusters'])\n",
    "print(extract_cluster_words(result['document'], result['clusters']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 29/29 [00:29<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create batches - not working ngl\n",
    "batch_size = 16 \n",
    "num_batches = len(gap_df) // batch_size + (1 if len(gap_df) % batch_size != 0 else 0)\n",
    "\n",
    "predictions = []\n",
    "num_rows = len(gap_df)\n",
    "num_batches = (num_rows + batch_size - 1) // batch_size\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_rows)\n",
    "        batch = gap_df.iloc[start_idx:end_idx]\n",
    "        for _, row in  batch.iterrows():\n",
    "            # print(row)\n",
    "            text = row['Text']\n",
    "            result = predictor.predict(document=text)\n",
    "            clusters = result['clusters']\n",
    "            tokens = result['document']\n",
    "\n",
    "            # Initialize coreference indicators for A and B to 0\n",
    "            a_coref, b_coref = 0, 0\n",
    "\n",
    "            # Find the cluster containing the pronoun\n",
    "            pronoun_offset = row['Pronoun-offset']\n",
    "            pronoun_length = len(row['Pronoun'])\n",
    "            pronoun_cluster = None\n",
    "\n",
    "            # Calculate the character offsets for each token\n",
    "            char_offsets = []\n",
    "            offset = 0\n",
    "            token_idx = 0\n",
    "            \n",
    "            tokens_dict = {}\n",
    "            for i, char in enumerate(text):\n",
    "                if char == ' ':\n",
    "                    continue\n",
    "                if text[i:i+len(tokens[token_idx])] == tokens[token_idx]:\n",
    "                    start_offset = i\n",
    "                    end_offset = i + len(tokens[token_idx]) - 1\n",
    "                    char_offsets.append((start_offset, end_offset))\n",
    "                    tokens_dict[ (start_offset, end_offset)] =tokens[token_idx]\n",
    "\n",
    "                    i = end_offset\n",
    "                    token_idx += 1\n",
    "                    if token_idx >= len(tokens):\n",
    "                        break\n",
    "                    \n",
    "            # print(tokens_dict)\n",
    "            # print('clusters',clusters)\n",
    "            # clusters = sorted(clusters, key=len) #Sort base on how cluster size, smaller is more important!\n",
    "            \n",
    "            # print('sorted clusters',clusters)\n",
    "            pronoun_clusters = []\n",
    "            for cluster in clusters:\n",
    "                for start, end in cluster:\n",
    "                    # print(char_offsets[start])\n",
    "                    try:\n",
    "                        start_offset, end_offset = char_offsets[start]\n",
    "                        if start_offset <= pronoun_offset and end_offset >= (pronoun_offset + pronoun_length - 1):\n",
    "                            pronoun_cluster = cluster\n",
    "                            # print(\"Pronoun cluster:\",cluster)\n",
    "                            pronoun_clusters.append(pronoun_cluster)\n",
    "                        # break\n",
    "                    except Exception as e:\n",
    "                        print(\"Warning - Error with selecting clusters, gonna ignore it and continue but be ware\")\n",
    "                        print(e)\n",
    "                        continue\n",
    "                # if pronoun_cluster:\n",
    "                #     break\n",
    "            # print(extract_cluster_words(tokens,[pronoun_cluster]))\n",
    "            # Check if 'A' or 'B' is in the same cluster as the pronoun\n",
    "            for pronoun_cluster in pronoun_clusters:\n",
    "                a_start, a_end = row['A-offset'], row['A-offset'] + len(row['A']) - 1\n",
    "                b_start, b_end = row['B-offset'], row['B-offset'] + len(row['B']) - 1\n",
    "                # print(\"A goal:\",a_start,a_end)\n",
    "                # print(\"B goal:\",b_start,b_end)\n",
    "                for start, end in pronoun_cluster:\n",
    "                    # print(\"start,end\",start,end)\n",
    "                    start_offset, _ = char_offsets[start]\n",
    "                    _, end_offset =char_offsets[end]\n",
    "                    # print(\"start,end char:\", start_offset, end_offset)\n",
    "                    if start_offset <= a_start and end_offset >= a_end and b_coref == 0:\n",
    "                        a_coref = 1\n",
    "                        break\n",
    "                    if start_offset <= b_start and end_offset >= b_end and a_coref == 0:\n",
    "                        b_coref = 1\n",
    "                        break\n",
    "\n",
    "            predictions.append({\n",
    "                'ID': row['ID'],\n",
    "                'A-coref': a_coref,\n",
    "                'B-coref': b_coref\n",
    "            })\n",
    "            # print(predictions)\n",
    "            # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scores\n",
    "Here we compute the f1 scores using gap_scorer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions have to be in shape: \\\\\n",
    "#        ID  A-coref  B-coref \\\\\n",
    "# 0  test-1        0        1 \\\\\n",
    "# 1  test-2        1        0 \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "golden_path = gap_file #this should match with the file we used to generate predictions\n",
    "# golden_path = 'gap-development.tsv'\n",
    "# golden_path = 'gap-validation.tsv'\n",
    "# golden_path = 'gap-test.tsv' \n",
    "predictions_path = golden_path.replace('gap','predictions')\n",
    "# predictions_path = 'predictions-test.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ID  A-coref  B-coref\n",
      "0  validation-1        1        0\n",
      "1  validation-2        0        1\n",
      "2  validation-3        0        1\n",
      "3  validation-4        1        0\n",
      "4  validation-5        0        1\n"
     ]
    }
   ],
   "source": [
    "# Convert the predictions dictionary to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "print(predictions_df[:5])\n",
    "# Convert 1 to True and 0 to False in the A-coref and B-coref columns\n",
    "predictions_df['A-coref'] = predictions_df['A-coref'].astype(bool)\n",
    "predictions_df['B-coref'] = predictions_df['B-coref'].astype(bool)\n",
    "# Save the DataFrame to a TSV file\n",
    "predictions_df.to_csv(predictions_path, sep='\\t', index=False)\n",
    "# Gold_annotations is a list of ground truth annotations from GAP\n",
    "# Run the scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected label! A-coref\n",
      "Unexpected label! B-coref\n",
      "Overall recall: 82.4 precision: 86.4 f1: 84.3\n",
      "\t\ttp 323\tfp 51\n",
      "\t\tfn 69\ttn 465\n",
      "Masculine recall: 78.7 precision: 81.3 f1: 80.0\n",
      "\t\ttp 148\tfp 34\n",
      "\t\tfn 40\ttn 232\n",
      "Feminine recall: 85.8 precision: 91.1 f1: 88.4\n",
      "\t\ttp 175\tfp 17\n",
      "\t\tfn 29\ttn 233\n",
      "Bias (F/M): 1.10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gap_scorer import run_scorer  \n",
    "scores = run_scorer(golden_path, predictions_path)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "with open(\"scores_\"+predictions_path.replace('.tsv',\".txt\"), \"w\") as f:\n",
    "    # json.dump(scores, f, indent=4)\n",
    "    f.writelines(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Cluster\n",
      "2 3\n",
      "['Vassey', \"'s\"]\n",
      "8 8\n",
      "['him']\n",
      "33 33\n",
      "['Vassey']\n",
      "36 36\n",
      "['his']\n",
      "39 39\n",
      "['Vassey']\n",
      "45 45\n",
      "['Vassey']\n",
      "53 53\n",
      "['him']\n",
      "63 64\n",
      "['Vassey', \"'s\"]\n",
      "New Cluster\n",
      "18 18\n",
      "['Denton']\n",
      "26 26\n",
      "['his']\n",
      "31 31\n",
      "['he']\n",
      "49 49\n",
      "['Denton']\n",
      "New Cluster\n",
      "12 12\n",
      "['Coakley']\n",
      "26 29\n",
      "['his', 'ex', '-', 'wife']\n",
      "[[\"Vassey 's\", 'him', 'Vassey', 'his', 'Vassey', 'Vassey', 'him', \"Vassey 's\"], ['Denton', 'his', 'he', 'Denton'], ['Coakley', 'his ex - wife']]\n"
     ]
    }
   ],
   "source": [
    "clu = []\n",
    "tks = result['document']\n",
    "total_clusters = []\n",
    "for cluster in result['clusters']:\n",
    "    print('New Cluster')\n",
    "    \n",
    "    txt_cluster = []\n",
    "    for start, end in cluster:\n",
    "        print(start,end)\n",
    "        print(tks[start:end+1])\n",
    "        txt_cluster.append(\" \".join(tks[start:end+1]) )\n",
    "    total_clusters.append(txt_cluster)\n",
    "print(total_clusters)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Pleasant, Start Offset: 0, End Offset: 7, Count 0\n",
      "Token: explains, Start Offset: 9, End Offset: 16, Count 1\n",
      "Token: Vassey, Start Offset: 18, End Offset: 23, Count 2\n",
      "Token: 's, Start Offset: 24, End Offset: 25, Count 3\n",
      "Token: guilty, Start Offset: 27, End Offset: 32, Count 4\n",
      "Token: conscience, Start Offset: 34, End Offset: 43, Count 5\n",
      "Token: may, Start Offset: 45, End Offset: 47, Count 6\n",
      "Token: lead, Start Offset: 49, End Offset: 52, Count 7\n",
      "Token: him, Start Offset: 54, End Offset: 56, Count 8\n",
      "Token: to, Start Offset: 58, End Offset: 59, Count 9\n",
      "Token: confess, Start Offset: 61, End Offset: 67, Count 10\n",
      "Token: to, Start Offset: 69, End Offset: 70, Count 11\n",
      "Token: Coakley, Start Offset: 72, End Offset: 78, Count 12\n",
      "Token: ., Start Offset: 79, End Offset: 79, Count 13\n",
      "Token: Pleasant, Start Offset: 81, End Offset: 88, Count 14\n",
      "Token: promises, Start Offset: 90, End Offset: 97, Count 15\n",
      "Token: to, Start Offset: 99, End Offset: 100, Count 16\n",
      "Token: help, Start Offset: 102, End Offset: 105, Count 17\n",
      "Token: Denton, Start Offset: 107, End Offset: 112, Count 18\n",
      "Token: renegotiate, Start Offset: 114, End Offset: 124, Count 19\n",
      "Token: the, Start Offset: 126, End Offset: 128, Count 20\n",
      "Token: terms, Start Offset: 130, End Offset: 134, Count 21\n",
      "Token: of, Start Offset: 136, End Offset: 137, Count 22\n",
      "Token: the, Start Offset: 139, End Offset: 141, Count 23\n",
      "Token: settlement, Start Offset: 143, End Offset: 152, Count 24\n",
      "Token: with, Start Offset: 154, End Offset: 157, Count 25\n",
      "Token: his, Start Offset: 159, End Offset: 161, Count 26\n",
      "Token: ex, Start Offset: 163, End Offset: 164, Count 27\n",
      "Token: -, Start Offset: 165, End Offset: 165, Count 28\n",
      "Token: wife, Start Offset: 166, End Offset: 169, Count 29\n",
      "Token: if, Start Offset: 167, End Offset: 168, Count 30\n",
      "Token: he, Start Offset: 174, End Offset: 175, Count 31\n",
      "Token: kills, Start Offset: 177, End Offset: 181, Count 32\n",
      "Token: Vassey, Start Offset: 183, End Offset: 188, Count 33\n",
      "Token: ., Start Offset: 189, End Offset: 189, Count 34\n",
      "Token: At, Start Offset: 191, End Offset: 192, Count 35\n",
      "Token: his, Start Offset: 194, End Offset: 196, Count 36\n",
      "Token: house, Start Offset: 198, End Offset: 202, Count 37\n",
      "Token: ,, Start Offset: 203, End Offset: 203, Count 38\n",
      "Token: Vassey, Start Offset: 205, End Offset: 210, Count 39\n",
      "Token: denies, Start Offset: 212, End Offset: 217, Count 40\n",
      "Token: the, Start Offset: 219, End Offset: 221, Count 41\n",
      "Token: rumors, Start Offset: 223, End Offset: 228, Count 42\n",
      "Token: ., Start Offset: 229, End Offset: 229, Count 43\n",
      "Token: As, Start Offset: 231, End Offset: 232, Count 44\n",
      "Token: Vassey, Start Offset: 234, End Offset: 239, Count 45\n",
      "Token: falls, Start Offset: 241, End Offset: 245, Count 46\n",
      "Token: asleep, Start Offset: 247, End Offset: 252, Count 47\n",
      "Token: ,, Start Offset: 253, End Offset: 253, Count 48\n",
      "Token: Denton, Start Offset: 255, End Offset: 260, Count 49\n",
      "Token: begins, Start Offset: 262, End Offset: 267, Count 50\n",
      "Token: to, Start Offset: 269, End Offset: 270, Count 51\n",
      "Token: suffocate, Start Offset: 272, End Offset: 280, Count 52\n",
      "Token: him, Start Offset: 282, End Offset: 284, Count 53\n",
      "Token: ,, Start Offset: 285, End Offset: 285, Count 54\n",
      "Token: only, Start Offset: 287, End Offset: 290, Count 55\n",
      "Token: to, Start Offset: 292, End Offset: 293, Count 56\n",
      "Token: be, Start Offset: 295, End Offset: 296, Count 57\n",
      "Token: interrupted, Start Offset: 298, End Offset: 308, Count 58\n",
      "Token: by, Start Offset: 310, End Offset: 311, Count 59\n",
      "Token: Charlotte, Start Offset: 313, End Offset: 321, Count 60\n",
      "Token: Boyd, Start Offset: 323, End Offset: 326, Count 61\n",
      "Token: ,, Start Offset: 327, End Offset: 327, Count 62\n",
      "Token: Vassey, Start Offset: 329, End Offset: 334, Count 63\n",
      "Token: 's, Start Offset: 335, End Offset: 336, Count 64\n",
      "Token: hospice, Start Offset: 338, End Offset: 344, Count 65\n",
      "Token: nurse, Start Offset: 346, End Offset: 350, Count 66\n",
      "Token: ., Start Offset: 351, End Offset: 351, Count 67\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "char_offsets = []\n",
    "offset = 0\n",
    "token_idx = 0\n",
    "\n",
    "# Iterate over the original text to calculate character-based offsets\n",
    "for i, char in enumerate(text):\n",
    "    # Skip spaces\n",
    "    if char == ' ':\n",
    "        continue\n",
    "    \n",
    "    # Check if the current character matches the start of the next token\n",
    "    if text[i:i+len(result['document'][token_idx])] == result['document'][token_idx]:\n",
    "        start_offset = i\n",
    "        end_offset = i + len(result['document'][token_idx]) - 1\n",
    "        char_offsets.append((start_offset, end_offset))\n",
    "        \n",
    "        # Move the pointer i to the end of the current token\n",
    "        i = end_offset\n",
    "        \n",
    "        # Move to the next token\n",
    "        token_idx += 1\n",
    "        \n",
    "        # Exit the loop if we've found all tokens\n",
    "        if token_idx >= len(result['document']):\n",
    "            break\n",
    "\n",
    "# Debugging: Print each token next to its offset\n",
    "count = 0\n",
    "for (start, end), token in zip(char_offsets, result['document']):\n",
    "    print(f\"Token: {token}, Start Offset: {start}, End Offset: {end}, Count {count}\")\n",
    "    count +=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 'validation-1', 'A-coref': False, 'B-coref': False},\n",
       " {'ID': 'validation-2', 'A-coref': False, 'B-coref': True},\n",
       " {'ID': 'validation-3', 'A-coref': False, 'B-coref': True},\n",
       " {'ID': 'validation-4', 'A-coref': True, 'B-coref': False},\n",
       " {'ID': 'validation-5', 'A-coref': False, 'B-coref': True}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_annotations = []\n",
    "\n",
    "for index, row in gap_df.iterrows():\n",
    "    gold_annotation = {\n",
    "        'ID': row['ID'],\n",
    "        'A-coref': True if row['A-coref'] == True else False,\n",
    "        'B-coref': True if row['B-coref'] == True else False,\n",
    "        # Add other fields as needed\n",
    "    }\n",
    "    gold_annotations.append(gold_annotation)\n",
    "gold_annotations[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
