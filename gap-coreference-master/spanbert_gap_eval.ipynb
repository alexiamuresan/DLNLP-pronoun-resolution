{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available, predictions will be faster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\pc-bae-2\\AppData\\Local\\Temp\\tmpc0nkkxmr\\config.json as plain json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from gap_scorer import run_scorer  \n",
    "from tqdm import tqdm\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available, predictions will be faster.\")\n",
    "else:\n",
    "    print(\"CUDA not available, predictions may be slower.\")\n",
    "# Initialize the SpanBERT predictor\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\",cuda_device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID                                               Text Pronoun  \\\n",
      "0  test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
      "1  test-2  Between the years 1979-1981, River won four lo...     him   \n",
      "2  test-3  Though his emigration from the country has aff...      He   \n",
      "3  test-4  At the trial, Pisciotta said: ``Those who have...     his   \n",
      "4  test-5  It is about a pair of United States Navy shore...     his   \n",
      "\n",
      "   Pronoun-offset             A  A-offset  A-coref                   B  \\\n",
      "0             383     Bob Suter       352    False              Dehner   \n",
      "1             430        Alonso       353     True  Alfredo Di St*fano   \n",
      "2             312  Ali Aladhadh       256     True              Saddam   \n",
      "3             526       Alliata       377    False           Pisciotta   \n",
      "4             406         Eddie       421     True         Rock Reilly   \n",
      "\n",
      "   B-offset  B-coref                                             URL  \n",
      "0       366     True      http://en.wikipedia.org/wiki/Jeremy_Dehner  \n",
      "1       390    False    http://en.wikipedia.org/wiki/Norberto_Alonso  \n",
      "2       295    False           http://en.wikipedia.org/wiki/Aladhadh  \n",
      "3       536     True  http://en.wikipedia.org/wiki/Gaspare_Pisciotta  \n",
      "4       559    False            http://en.wikipedia.org/wiki/Chasers  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read GAP data\n",
    "main_path = \"../gap-coreference-master/gap-coreference-master\"  \n",
    "main_path = \".\" \n",
    "gap_file =  \"gap-test.tsv\"\n",
    "gap_development_path = os.path.join(main_path,gap_file)\n",
    "gap_df = pd.read_csv(gap_development_path, delimiter='\\t')\n",
    "print(gap_df[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                                    development-3\n",
      "Text              He had been reelected to Congress, but resigne...\n",
      "Pronoun                                                         his\n",
      "Pronoun-offset                                                  265\n",
      "A                                                           Angeloz\n",
      "A-offset                                                        173\n",
      "A-coref                                                       False\n",
      "B                                                        De la Sota\n",
      "B-offset                                                        246\n",
      "B-coref                                                        True\n",
      "URL               http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...\n",
      "Name: 2, dtype: object\n",
      "[[[0, 0], [20, 22], [49, 51], [54, 54], [98, 101]], [[32, 32], [41, 43], [47, 47]], [[30, 30], [68, 68]], [[28, 28], [87, 87]], [[5, 5], [108, 108]]]\n",
      "He had been reelected to Congress, but resigned in 1990 to accept a post as Ambassador to Brazil. De la Sota again ran for governor of C*rdoba in 1991. Defeated by Governor Angeloz by over 15%, this latter setback was significant because it cost De la Sota much of his support within the Justicialist Party (which was flush with victory in the 1991 mid-terms), leading to President Carlos Menem 's endorsement of a separate party list in C*rdoba for the 1993 mid-term elections, and to De la Sota's failure to regain a seat in Congress.\n",
      "[He] had been reelected to [Congress] , but resigned in 1990 to accept a post as Ambassador to Brazil . [De la Sota] again ran for governor of [C*rdoba] in [1991] . [Defeated] by Governor Angeloz by over 15 % , [this latter setback] was significant because [it] cost [De la Sota] much of [his] support within the Justicialist Party ( which was flush with victory in the [1991] mid - terms ) , leading to President Carlos Menem 's endorsement of a separate party list in [C*rdoba] for the 1993 mid - term elections , and to [De la Sota 's] failure to regain a seat in [Congress] .\n"
     ]
    }
   ],
   "source": [
    "# Sample output from AllenNLP coref model\n",
    "example = gap_df.iloc[2]\n",
    "text = example['Text']\n",
    "print(example)\n",
    "result = predictor.predict(document=text)\n",
    "print(result['clusters'])\n",
    "# Initialize an empty list with placeholders\n",
    "text_visual = ['_'] * len(result['document'])\n",
    "\n",
    "# Fill in the placeholders with tokens\n",
    "for i, token in enumerate(result['document']):\n",
    "    text_visual[i] = token\n",
    "\n",
    "# Add brackets for coref clusters\n",
    "for cluster in result['clusters']:\n",
    "    for start, end in cluster:\n",
    "        text_visual[start] = '[' + text_visual[start]\n",
    "        text_visual[end] = text_visual[end] + ']'\n",
    "\n",
    "# Combine into a string\n",
    "text_visual_str = ' '.join(text_visual)\n",
    "print(text)\n",
    "print(text_visual_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Simon', 'Simon', 'he', \"Simon 's\", \"Simon 's\"],\n",
       " [\"Cheryl Cassidy , Pauline 's friend and also a year 11 pupil in Simon 's class\",\n",
       "  'her',\n",
       "  'her',\n",
       "  'her'],\n",
       " ['her boyfriend', 'he', 'him'],\n",
       " ['Dumped', 'this'],\n",
       " [\"Pauline 's\", 'her friend Pauline']]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function to extract words that correspond to each cluster\n",
    "def extract_cluster_words(tokenized_document, clusters):\n",
    "    total_clusters = []\n",
    "    for cluster in clusters:\n",
    "        txt_cluster = []\n",
    "        for start, end in cluster:\n",
    "            cluster_tokens = tokenized_document[start:end+1]\n",
    "            txt_cluster.append(\" \".join(cluster_tokens))\n",
    "        total_clusters.append(txt_cluster)\n",
    "    return total_clusters\n",
    "\n",
    "# Example \n",
    "tokenized_document = ['It', 'was', 'reported', 'that', 'John', 'and', 'Jane', 'were', 'together', '.', 'He', 'said', 'it', 'was', 'true', '.']\n",
    "clusters = [[[4, 4], [10, 10]], [[5, 5], [12, 12]]]\n",
    "print(result['clusters'])\n",
    "print(extract_cluster_words(result['document'], result['clusters']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 125/125 [02:18<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create batches - not working ngl\n",
    "batch_size = 16 \n",
    "num_batches = len(gap_df) // batch_size + (1 if len(gap_df) % batch_size != 0 else 0)\n",
    "\n",
    "predictions = []\n",
    "num_rows = len(gap_df)\n",
    "num_batches = (num_rows + batch_size - 1) // batch_size\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_rows)\n",
    "        batch = gap_df.iloc[start_idx:end_idx]\n",
    "        for _, row in  batch.iterrows():\n",
    "            # print(row)\n",
    "            text = row['Text']\n",
    "            result = predictor.predict(document=text)\n",
    "            clusters = result['clusters']\n",
    "            tokens = result['document']\n",
    "\n",
    "            # Initialize coreference indicators for A and B to 0\n",
    "            a_coref, b_coref = 0, 0\n",
    "\n",
    "            # Find the cluster containing the pronoun\n",
    "            pronoun_offset = row['Pronoun-offset']\n",
    "            pronoun_length = len(row['Pronoun'])\n",
    "            pronoun_cluster = None\n",
    "\n",
    "            # Calculate the character offsets for each token\n",
    "            char_offsets = []\n",
    "            offset = 0\n",
    "            token_idx = 0\n",
    "            \n",
    "            tokens_dict = {}\n",
    "            for i, char in enumerate(text):\n",
    "                if char == ' ':\n",
    "                    continue\n",
    "                if text[i:i+len(tokens[token_idx])] == tokens[token_idx]:\n",
    "                    start_offset = i\n",
    "                    end_offset = i + len(tokens[token_idx]) - 1\n",
    "                    char_offsets.append((start_offset, end_offset))\n",
    "                    tokens_dict[ (start_offset, end_offset)] =tokens[token_idx]\n",
    "\n",
    "                    i = end_offset\n",
    "                    token_idx += 1\n",
    "                    if token_idx >= len(tokens):\n",
    "                        break\n",
    "                    \n",
    "            # print(tokens_dict)\n",
    "            # print('clusters',clusters)\n",
    "            # clusters = sorted(clusters, key=len) #Sort base on how cluster size, smaller is more important!\n",
    "            \n",
    "            # print('sorted clusters',clusters)\n",
    "            pronoun_clusters = []\n",
    "            for cluster in clusters:\n",
    "                for start, end in cluster:\n",
    "                    # print(char_offsets[start])\n",
    "                    try:\n",
    "                        start_offset, end_offset = char_offsets[start]\n",
    "                        if start_offset <= pronoun_offset and end_offset >= (pronoun_offset + pronoun_length - 1):\n",
    "                            pronoun_cluster = cluster\n",
    "                            # print(\"Pronoun cluster:\",cluster)\n",
    "                            pronoun_clusters.append(pronoun_cluster)\n",
    "                        # break\n",
    "                    except Exception as e:\n",
    "                        print(\"Warning - Error with selecting clusters, gonna ignore it and continue but be ware\")\n",
    "                        print(e)\n",
    "                        continue\n",
    "                # if pronoun_cluster:\n",
    "                #     break\n",
    "            # print(extract_cluster_words(tokens,[pronoun_cluster]))\n",
    "            # Check if 'A' or 'B' is in the same cluster as the pronoun\n",
    "            for pronoun_cluster in pronoun_clusters:\n",
    "                a_start, a_end = row['A-offset'], row['A-offset'] + len(row['A']) - 1\n",
    "                b_start, b_end = row['B-offset'], row['B-offset'] + len(row['B']) - 1\n",
    "                # print(\"A goal:\",a_start,a_end)\n",
    "                # print(\"B goal:\",b_start,b_end)\n",
    "                for start, end in pronoun_cluster:\n",
    "                    # print(\"start,end\",start,end)\n",
    "                    start_offset, _ = char_offsets[start]\n",
    "                    _, end_offset =char_offsets[end]\n",
    "                    # print(\"start,end char:\", start_offset, end_offset)\n",
    "                    if start_offset <= a_start and end_offset >= a_end and b_coref == 0:\n",
    "                        a_coref = 1\n",
    "                        break\n",
    "                    if start_offset <= b_start and end_offset >= b_end and a_coref == 0:\n",
    "                        b_coref = 1\n",
    "                        break\n",
    "\n",
    "            predictions.append({\n",
    "                'ID': row['ID'],\n",
    "                'A-coref': a_coref,\n",
    "                'B-coref': b_coref\n",
    "            })\n",
    "            # print(predictions)\n",
    "            # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the predictions dictionary to a DataFrame\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "# Convert 1 to True and 0 to False in the A-coref and B-coref columns\n",
    "predictions_df['A-coref'] = predictions_df['A-coref'].astype(bool)\n",
    "predictions_df['B-coref'] = predictions_df['B-coref'].astype(bool)\n",
    "# Save the DataFrame to a TSV file\n",
    "predictions_df.to_csv('predictions-test.tsv', sep='\\t', index=False)\n",
    "# Gold_annotations is a list of ground truth annotations from GAP\n",
    "# Run the scorer\n",
    "scores = run_scorer('gap-development.tsv', 'predictions-test.tsv')\n",
    "\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Cluster\n",
      "0 0\n",
      "['He']\n",
      "20 22\n",
      "['De', 'la', 'Sota']\n",
      "49 51\n",
      "['De', 'la', 'Sota']\n",
      "54 54\n",
      "['his']\n",
      "98 101\n",
      "['De', 'la', 'Sota', \"'s\"]\n",
      "New Cluster\n",
      "32 32\n",
      "['Defeated']\n",
      "41 43\n",
      "['this', 'latter', 'setback']\n",
      "47 47\n",
      "['it']\n",
      "New Cluster\n",
      "30 30\n",
      "['1991']\n",
      "68 68\n",
      "['1991']\n",
      "New Cluster\n",
      "28 28\n",
      "['C*rdoba']\n",
      "87 87\n",
      "['C*rdoba']\n",
      "New Cluster\n",
      "5 5\n",
      "['Congress']\n",
      "108 108\n",
      "['Congress']\n",
      "[['He', 'De la Sota', 'De la Sota', 'his', \"De la Sota 's\"], ['Defeated', 'this latter setback', 'it'], ['1991', '1991'], ['C*rdoba', 'C*rdoba'], ['Congress', 'Congress']]\n"
     ]
    }
   ],
   "source": [
    "clu = []\n",
    "tks = result['document']\n",
    "total_clusters = []\n",
    "for cluster in result['clusters']:\n",
    "    print('New Cluster')\n",
    "    \n",
    "    txt_cluster = []\n",
    "    for start, end in cluster:\n",
    "        print(start,end)\n",
    "        print(tks[start:end+1])\n",
    "        txt_cluster.append(\" \".join(tks[start:end+1]) )\n",
    "    total_clusters.append(txt_cluster)\n",
    "print(total_clusters)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: He, Start Offset: 0, End Offset: 1, Count 0\n",
      "Token: had, Start Offset: 3, End Offset: 5, Count 1\n",
      "Token: been, Start Offset: 7, End Offset: 10, Count 2\n",
      "Token: reelected, Start Offset: 12, End Offset: 20, Count 3\n",
      "Token: to, Start Offset: 22, End Offset: 23, Count 4\n",
      "Token: Congress, Start Offset: 25, End Offset: 32, Count 5\n",
      "Token: ,, Start Offset: 33, End Offset: 33, Count 6\n",
      "Token: but, Start Offset: 35, End Offset: 37, Count 7\n",
      "Token: resigned, Start Offset: 39, End Offset: 46, Count 8\n",
      "Token: in, Start Offset: 48, End Offset: 49, Count 9\n",
      "Token: 1990, Start Offset: 51, End Offset: 54, Count 10\n",
      "Token: to, Start Offset: 56, End Offset: 57, Count 11\n",
      "Token: accept, Start Offset: 59, End Offset: 64, Count 12\n",
      "Token: a, Start Offset: 66, End Offset: 66, Count 13\n",
      "Token: post, Start Offset: 68, End Offset: 71, Count 14\n",
      "Token: as, Start Offset: 73, End Offset: 74, Count 15\n",
      "Token: Ambassador, Start Offset: 76, End Offset: 85, Count 16\n",
      "Token: to, Start Offset: 87, End Offset: 88, Count 17\n",
      "Token: Brazil, Start Offset: 90, End Offset: 95, Count 18\n",
      "Token: ., Start Offset: 96, End Offset: 96, Count 19\n",
      "Token: De, Start Offset: 98, End Offset: 99, Count 20\n",
      "Token: la, Start Offset: 101, End Offset: 102, Count 21\n",
      "Token: Sota, Start Offset: 104, End Offset: 107, Count 22\n",
      "Token: again, Start Offset: 109, End Offset: 113, Count 23\n",
      "Token: ran, Start Offset: 115, End Offset: 117, Count 24\n",
      "Token: for, Start Offset: 119, End Offset: 121, Count 25\n",
      "Token: governor, Start Offset: 123, End Offset: 130, Count 26\n",
      "Token: of, Start Offset: 132, End Offset: 133, Count 27\n",
      "Token: C*rdoba, Start Offset: 135, End Offset: 141, Count 28\n",
      "Token: in, Start Offset: 143, End Offset: 144, Count 29\n",
      "Token: 1991, Start Offset: 146, End Offset: 149, Count 30\n",
      "Token: ., Start Offset: 150, End Offset: 150, Count 31\n",
      "Token: Defeated, Start Offset: 152, End Offset: 159, Count 32\n",
      "Token: by, Start Offset: 161, End Offset: 162, Count 33\n",
      "Token: Governor, Start Offset: 164, End Offset: 171, Count 34\n",
      "Token: Angeloz, Start Offset: 173, End Offset: 179, Count 35\n",
      "Token: by, Start Offset: 181, End Offset: 182, Count 36\n",
      "Token: over, Start Offset: 184, End Offset: 187, Count 37\n",
      "Token: 15, Start Offset: 189, End Offset: 190, Count 38\n",
      "Token: %, Start Offset: 191, End Offset: 191, Count 39\n",
      "Token: ,, Start Offset: 192, End Offset: 192, Count 40\n",
      "Token: this, Start Offset: 194, End Offset: 197, Count 41\n",
      "Token: latter, Start Offset: 199, End Offset: 204, Count 42\n",
      "Token: setback, Start Offset: 206, End Offset: 212, Count 43\n",
      "Token: was, Start Offset: 214, End Offset: 216, Count 44\n",
      "Token: significant, Start Offset: 218, End Offset: 228, Count 45\n",
      "Token: because, Start Offset: 230, End Offset: 236, Count 46\n",
      "Token: it, Start Offset: 238, End Offset: 239, Count 47\n",
      "Token: cost, Start Offset: 241, End Offset: 244, Count 48\n",
      "Token: De, Start Offset: 246, End Offset: 247, Count 49\n",
      "Token: la, Start Offset: 249, End Offset: 250, Count 50\n",
      "Token: Sota, Start Offset: 252, End Offset: 255, Count 51\n",
      "Token: much, Start Offset: 257, End Offset: 260, Count 52\n",
      "Token: of, Start Offset: 262, End Offset: 263, Count 53\n",
      "Token: his, Start Offset: 265, End Offset: 267, Count 54\n",
      "Token: support, Start Offset: 269, End Offset: 275, Count 55\n",
      "Token: within, Start Offset: 277, End Offset: 282, Count 56\n",
      "Token: the, Start Offset: 284, End Offset: 286, Count 57\n",
      "Token: Justicialist, Start Offset: 288, End Offset: 299, Count 58\n",
      "Token: Party, Start Offset: 301, End Offset: 305, Count 59\n",
      "Token: (, Start Offset: 307, End Offset: 307, Count 60\n",
      "Token: which, Start Offset: 308, End Offset: 312, Count 61\n",
      "Token: was, Start Offset: 314, End Offset: 316, Count 62\n",
      "Token: flush, Start Offset: 318, End Offset: 322, Count 63\n",
      "Token: with, Start Offset: 324, End Offset: 327, Count 64\n",
      "Token: victory, Start Offset: 329, End Offset: 335, Count 65\n",
      "Token: in, Start Offset: 337, End Offset: 338, Count 66\n",
      "Token: the, Start Offset: 340, End Offset: 342, Count 67\n",
      "Token: 1991, Start Offset: 344, End Offset: 347, Count 68\n",
      "Token: mid, Start Offset: 349, End Offset: 351, Count 69\n",
      "Token: -, Start Offset: 352, End Offset: 352, Count 70\n",
      "Token: terms, Start Offset: 353, End Offset: 357, Count 71\n",
      "Token: ), Start Offset: 358, End Offset: 358, Count 72\n",
      "Token: ,, Start Offset: 359, End Offset: 359, Count 73\n",
      "Token: leading, Start Offset: 361, End Offset: 367, Count 74\n",
      "Token: to, Start Offset: 369, End Offset: 370, Count 75\n",
      "Token: President, Start Offset: 372, End Offset: 380, Count 76\n",
      "Token: Carlos, Start Offset: 382, End Offset: 387, Count 77\n",
      "Token: Menem, Start Offset: 389, End Offset: 393, Count 78\n",
      "Token: 's, Start Offset: 395, End Offset: 396, Count 79\n",
      "Token: endorsement, Start Offset: 398, End Offset: 408, Count 80\n",
      "Token: of, Start Offset: 410, End Offset: 411, Count 81\n",
      "Token: a, Start Offset: 413, End Offset: 413, Count 82\n",
      "Token: separate, Start Offset: 415, End Offset: 422, Count 83\n",
      "Token: party, Start Offset: 424, End Offset: 428, Count 84\n",
      "Token: list, Start Offset: 430, End Offset: 433, Count 85\n",
      "Token: in, Start Offset: 435, End Offset: 436, Count 86\n",
      "Token: C*rdoba, Start Offset: 438, End Offset: 444, Count 87\n",
      "Token: for, Start Offset: 446, End Offset: 448, Count 88\n",
      "Token: the, Start Offset: 450, End Offset: 452, Count 89\n",
      "Token: 1993, Start Offset: 454, End Offset: 457, Count 90\n",
      "Token: mid, Start Offset: 459, End Offset: 461, Count 91\n",
      "Token: -, Start Offset: 462, End Offset: 462, Count 92\n",
      "Token: term, Start Offset: 463, End Offset: 466, Count 93\n",
      "Token: elections, Start Offset: 468, End Offset: 476, Count 94\n",
      "Token: ,, Start Offset: 477, End Offset: 477, Count 95\n",
      "Token: and, Start Offset: 479, End Offset: 481, Count 96\n",
      "Token: to, Start Offset: 483, End Offset: 484, Count 97\n",
      "Token: De, Start Offset: 486, End Offset: 487, Count 98\n",
      "Token: la, Start Offset: 489, End Offset: 490, Count 99\n",
      "Token: Sota, Start Offset: 492, End Offset: 495, Count 100\n",
      "Token: 's, Start Offset: 496, End Offset: 497, Count 101\n",
      "Token: failure, Start Offset: 499, End Offset: 505, Count 102\n",
      "Token: to, Start Offset: 507, End Offset: 508, Count 103\n",
      "Token: regain, Start Offset: 510, End Offset: 515, Count 104\n",
      "Token: a, Start Offset: 513, End Offset: 513, Count 105\n",
      "Token: seat, Start Offset: 519, End Offset: 522, Count 106\n",
      "Token: in, Start Offset: 524, End Offset: 525, Count 107\n",
      "Token: Congress, Start Offset: 527, End Offset: 534, Count 108\n",
      "Token: ., Start Offset: 535, End Offset: 535, Count 109\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "char_offsets = []\n",
    "offset = 0\n",
    "token_idx = 0\n",
    "\n",
    "# Iterate over the original text to calculate character-based offsets\n",
    "for i, char in enumerate(text):\n",
    "    # Skip spaces\n",
    "    if char == ' ':\n",
    "        continue\n",
    "    \n",
    "    # Check if the current character matches the start of the next token\n",
    "    if text[i:i+len(result['document'][token_idx])] == result['document'][token_idx]:\n",
    "        start_offset = i\n",
    "        end_offset = i + len(result['document'][token_idx]) - 1\n",
    "        char_offsets.append((start_offset, end_offset))\n",
    "        \n",
    "        # Move the pointer i to the end of the current token\n",
    "        i = end_offset\n",
    "        \n",
    "        # Move to the next token\n",
    "        token_idx += 1\n",
    "        \n",
    "        # Exit the loop if we've found all tokens\n",
    "        if token_idx >= len(result['document']):\n",
    "            break\n",
    "\n",
    "# Debugging: Print each token next to its offset\n",
    "count = 0\n",
    "for (start, end), token in zip(char_offsets, result['document']):\n",
    "    print(f\"Token: {token}, Start Offset: {start}, End Offset: {end}, Count {count}\")\n",
    "    count +=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 'development-1', 'A-coref': True, 'B-coref': False},\n",
       " {'ID': 'development-2', 'A-coref': True, 'B-coref': False},\n",
       " {'ID': 'development-3', 'A-coref': False, 'B-coref': True},\n",
       " {'ID': 'development-4', 'A-coref': False, 'B-coref': True},\n",
       " {'ID': 'development-5', 'A-coref': False, 'B-coref': True}]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_annotations = []\n",
    "\n",
    "for index, row in gap_df.iterrows():\n",
    "    gold_annotation = {\n",
    "        'ID': row['ID'],\n",
    "        'A-coref': True if row['A-coref'] == True else False,\n",
    "        'B-coref': True if row['B-coref'] == True else False,\n",
    "        # Add other fields as needed\n",
    "    }\n",
    "    gold_annotations.append(gold_annotation)\n",
    "gold_annotations[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
