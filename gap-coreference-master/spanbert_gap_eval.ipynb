{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpanBert GAP Evaluation\n",
    "\n",
    "- We load the spanbert model or download it if its missing. \n",
    "- We do data processing for GAP \n",
    "- Generate predictions for GAP data \n",
    "- Use official scorer from GAP repository to compute F1 and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available, predictions will be faster.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from gap_scorer import run_scorer  \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# Check for CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available, predictions will be faster.\")\n",
    "else:\n",
    "    print(\"CUDA not available, predictions may be slower.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model or download if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\pc-bae-2\\AppData\\Local\\Temp\\tmpy_0x34aq\\config.json as plain json\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SpanBERT predictor\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Path to the file\n",
    "path_spanbert = 'spanbert_local/'\n",
    "filename = \"coref-spanbert-large-2021.03.10.tar.gz\"\n",
    "\n",
    "\n",
    "save_path = os.path.join(path_spanbert, filename)\n",
    "if os.path.exists(save_path):\n",
    "    predictor = Predictor.from_path(save_path,cuda_device=0)\n",
    "else:\n",
    "        # Create directory if it doesn't exist\n",
    "    if not os.path.exists(path_spanbert):\n",
    "        os.makedirs(path_spanbert)\n",
    "        # Full path to save the file\n",
    "\n",
    "    # Download the file\n",
    "    urllib.request.urlretrieve(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\", save_path)\n",
    "    predictor = Predictor.from_path(save_path,cuda_device=0)\n",
    "\n",
    "    # predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\",cuda_device=0) #use this if urllib didnt work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "Data is in the same folder as notebook so should work as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID                                               Text Pronoun  \\\n",
      "0  test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
      "1  test-2  Between the years 1979-1981, River won four lo...     him   \n",
      "2  test-3  Though his emigration from the country has aff...      He   \n",
      "3  test-4  At the trial, Pisciotta said: ``Those who have...     his   \n",
      "4  test-5  It is about a pair of United States Navy shore...     his   \n",
      "\n",
      "   Pronoun-offset             A  A-offset  A-coref                   B  \\\n",
      "0             383     Bob Suter       352    False              Dehner   \n",
      "1             430        Alonso       353     True  Alfredo Di St*fano   \n",
      "2             312  Ali Aladhadh       256     True              Saddam   \n",
      "3             526       Alliata       377    False           Pisciotta   \n",
      "4             406         Eddie       421     True         Rock Reilly   \n",
      "\n",
      "   B-offset  B-coref                                             URL  \n",
      "0       366     True      http://en.wikipedia.org/wiki/Jeremy_Dehner  \n",
      "1       390    False    http://en.wikipedia.org/wiki/Norberto_Alonso  \n",
      "2       295    False           http://en.wikipedia.org/wiki/Aladhadh  \n",
      "3       536     True  http://en.wikipedia.org/wiki/Gaspare_Pisciotta  \n",
      "4       559    False            http://en.wikipedia.org/wiki/Chasers  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read GAP data\n",
    "main_path = \"../gap-coreference-master/gap-coreference-master\"  \n",
    "main_path = \".\" \n",
    "gap_file =  \"gap-test.tsv\"\n",
    "# gap_file = \"gap-test-gn.tsv\"\n",
    "gap_development_path = os.path.join(main_path,gap_file)\n",
    "gap_df = pd.read_csv(gap_development_path, delimiter='\\t')\n",
    "print(gap_df[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model output example\n",
    "We compute an example and process it to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                                           test-3\n",
      "Text              Though his emigration from the country has aff...\n",
      "Pronoun                                                          He\n",
      "Pronoun-offset                                                  312\n",
      "A                                                      Ali Aladhadh\n",
      "A-offset                                                        256\n",
      "A-coref                                                        True\n",
      "B                                                            Saddam\n",
      "B-offset                                                        295\n",
      "B-coref                                                       False\n",
      "URL                           http://en.wikipedia.org/wiki/Aladhadh\n",
      "Name: 2, dtype: object\n",
      "[[[1, 1], [8, 8], [12, 12]], [[19, 20], [39, 40]], [[32, 34], [42, 47], [49, 50], [61, 61], [65, 65], [69, 69]]]\n",
      "Though his emigration from the country has affected his leadership status, Kamel is still a respected elder of the clan. After the fall of Hussien's regime, many considered Dr. Ali Aladhadh a candidate to lead the clan. A contributor to Iraq's liberation, Ali Aladhadh and a long time oppose to Saddam's regime. He was ambushed with his pregnant wife on his way to the hospital in 2006 by Iraqi insurgents.\n",
      "Though [his] emigration from the country has affected [his] leadership status , [Kamel] is still a respected elder of [the clan] . After the fall of Hussien 's regime , many considered [Dr. Ali Aladhadh] a candidate to lead [the clan] . [A contributor to Iraq 's liberation] , [Ali Aladhadh] and a long time oppose to Saddam 's regime . [He] was ambushed with [his] pregnant wife on [his] way to the hospital in 2006 by Iraqi insurgents .\n"
     ]
    }
   ],
   "source": [
    "# Sample output from AllenNLP coref model\n",
    "example = gap_df.iloc[2]\n",
    "text = example['Text']\n",
    "print(example)\n",
    "result = predictor.predict(document=text)\n",
    "print(result['clusters'])\n",
    "# Initialize an empty list with placeholders\n",
    "text_visual = ['_'] * len(result['document'])\n",
    "\n",
    "# Fill in the placeholders with tokens\n",
    "for i, token in enumerate(result['document']):\n",
    "    text_visual[i] = token\n",
    "\n",
    "# Add brackets for coref clusters\n",
    "for cluster in result['clusters']:\n",
    "    for start, end in cluster:\n",
    "        text_visual[start] = '[' + text_visual[start]\n",
    "        text_visual[end] = text_visual[end] + ']'\n",
    "\n",
    "# Combine into a string\n",
    "text_visual_str = ' '.join(text_visual)\n",
    "print(text)\n",
    "print(text_visual_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 1], [8, 8], [12, 12]], [[19, 20], [39, 40]], [[32, 34], [42, 47], [49, 50], [61, 61], [65, 65], [69, 69]]]\n",
      "[['his', 'his', 'Kamel'], ['the clan', 'the clan'], ['Dr. Ali Aladhadh', \"A contributor to Iraq 's liberation\", 'Ali Aladhadh', 'He', 'his', 'his']]\n"
     ]
    }
   ],
   "source": [
    "# Extract words that correspond to each cluster\n",
    "def extract_cluster_words(tokenized_document, clusters):\n",
    "    total_clusters = []\n",
    "    for cluster in clusters:\n",
    "        txt_cluster = []\n",
    "        for start, end in cluster:\n",
    "            cluster_tokens = tokenized_document[start:end+1]\n",
    "            txt_cluster.append(\" \".join(cluster_tokens))\n",
    "        total_clusters.append(txt_cluster)\n",
    "    return total_clusters\n",
    "\n",
    "# Example \n",
    "tokenized_document = ['It', 'was', 'reported', 'that', 'John', 'and', 'Jane', 'were', 'together', '.', 'He', 'said', 'it', 'was', 'true', '.']\n",
    "clusters = [[[4, 4], [10, 10]], [[5, 5], [12, 12]]]\n",
    "print(result['clusters'])\n",
    "print(extract_cluster_words(result['document'], result['clusters']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation loop\n",
    "Here is where we loop through the data and generate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  83%|████████▎ | 104/125 [01:51<00:21,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Error with selecting clusters, gonna ignore it and continue but be ware\n",
      "list index out of range\n",
      "Warning - Error with selecting clusters, gonna ignore it and continue but be ware\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 125/125 [02:12<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create batches - not working ngl\n",
    "batch_size = 16 \n",
    "num_batches = len(gap_df) // batch_size + (1 if len(gap_df) % batch_size != 0 else 0)\n",
    "\n",
    "predictions = []\n",
    "num_rows = len(gap_df)\n",
    "num_batches = (num_rows + batch_size - 1) // batch_size\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_rows)\n",
    "        batch = gap_df.iloc[start_idx:end_idx]\n",
    "        for _, row in  batch.iterrows():\n",
    "            # print(row)\n",
    "            text = row['Text']\n",
    "            result = predictor.predict(document=text)\n",
    "            clusters = result['clusters']\n",
    "            tokens = result['document']\n",
    "\n",
    "            # Initialize coreference indicators for A and B to 0\n",
    "            a_coref, b_coref = 0, 0\n",
    "\n",
    "            # Find the cluster containing the pronoun\n",
    "            pronoun_offset = row['Pronoun-offset']\n",
    "            pronoun_length = len(row['Pronoun'])\n",
    "            pronoun_cluster = None\n",
    "\n",
    "            # Calculate the character offsets for each token\n",
    "            char_offsets = []\n",
    "            offset = 0\n",
    "            token_idx = 0\n",
    "            \n",
    "            tokens_dict = {}\n",
    "            for i, char in enumerate(text):\n",
    "                if char == ' ':\n",
    "                    continue\n",
    "                if text[i:i+len(tokens[token_idx])] == tokens[token_idx]:\n",
    "                    start_offset = i\n",
    "                    end_offset = i + len(tokens[token_idx]) - 1\n",
    "                    char_offsets.append((start_offset, end_offset))\n",
    "                    tokens_dict[ (start_offset, end_offset)] =tokens[token_idx]\n",
    "\n",
    "                    i = end_offset\n",
    "                    token_idx += 1\n",
    "                    if token_idx >= len(tokens):\n",
    "                        break\n",
    "                    \n",
    "            # print(tokens_dict)\n",
    "            # print('clusters',clusters)\n",
    "            # clusters = sorted(clusters, key=len) #Sort base on how cluster size, smaller is more important!\n",
    "            \n",
    "            # print('sorted clusters',clusters)\n",
    "            pronoun_clusters = []\n",
    "            for cluster in clusters:\n",
    "                for start, end in cluster:\n",
    "                    # print(char_offsets[start])\n",
    "                    try:\n",
    "                        start_offset, end_offset = char_offsets[start]\n",
    "                        if start_offset <= pronoun_offset and end_offset >= (pronoun_offset + pronoun_length - 1):\n",
    "                            pronoun_cluster = cluster\n",
    "                            # print(\"Pronoun cluster:\",cluster)\n",
    "                            pronoun_clusters.append(pronoun_cluster)\n",
    "                        # break\n",
    "                    except Exception as e:\n",
    "                        print(\"Warning - Error with selecting clusters, gonna ignore it and continue but be ware\")\n",
    "                        print(e)\n",
    "                        continue\n",
    "                # if pronoun_cluster:\n",
    "                #     break\n",
    "            # print(extract_cluster_words(tokens,[pronoun_cluster]))\n",
    "            # Check if 'A' or 'B' is in the same cluster as the pronoun\n",
    "            for pronoun_cluster in pronoun_clusters:\n",
    "                a_start, a_end = row['A-offset'], row['A-offset'] + len(row['A']) - 1\n",
    "                b_start, b_end = row['B-offset'], row['B-offset'] + len(row['B']) - 1\n",
    "                # print(\"A goal:\",a_start,a_end)\n",
    "                # print(\"B goal:\",b_start,b_end)\n",
    "                for start, end in pronoun_cluster:\n",
    "                    # print(\"start,end\",start,end)\n",
    "                    start_offset, _ = char_offsets[start]\n",
    "                    _, end_offset =char_offsets[end]\n",
    "                    # print(\"start,end char:\", start_offset, end_offset)\n",
    "                    if start_offset <= a_start and end_offset >= a_end and b_coref == 0:\n",
    "                        a_coref = 1\n",
    "                        break\n",
    "                    if start_offset <= b_start and end_offset >= b_end and a_coref == 0:\n",
    "                        b_coref = 1\n",
    "                        break\n",
    "\n",
    "            predictions.append({\n",
    "                'ID': row['ID'],\n",
    "                'A-coref': a_coref,\n",
    "                'B-coref': b_coref\n",
    "            })\n",
    "            # print(predictions)\n",
    "            # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scores\n",
    "Here we compute the f1 scores using gap_scorer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions have to be in shape: \\\\\n",
    "#        ID  A-coref  B-coref \\\\\n",
    "# 0  test-1        0        1 \\\\\n",
    "# 1  test-2        1        0 \\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "golden_path = gap_file #this should match with the file we used to generate predictions on top of notebook\n",
    "# golden_path = 'gap-development.tsv'\n",
    "# golden_path = 'gap-validation.tsv'\n",
    "# golden_path = 'gap-test.tsv' \n",
    "predictions_path = golden_path.replace('gap','predictions')\n",
    "# predictions_path = 'predictions-test.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID  A-coref  B-coref\n",
      "0  test-1        0        1\n",
      "1  test-2        1        0\n",
      "2  test-3        1        0\n",
      "3  test-4        0        1\n",
      "4  test-5        1        0\n"
     ]
    }
   ],
   "source": [
    "# dictnary to df\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "print(predictions_df[:5])\n",
    "# Convert 1 to True and 0 to False in the A-coref and B-coref columns\n",
    "predictions_df['A-coref'] = predictions_df['A-coref'].astype(bool)\n",
    "predictions_df['B-coref'] = predictions_df['B-coref'].astype(bool)\n",
    "# DataFrame to a TSV file\n",
    "predictions_df.to_csv(predictions_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected label! A-coref\n",
      "Unexpected label! B-coref\n",
      "Overall recall: 85.4 precision: 90.8 f1: 88.0\n",
      "\t\ttp 1514\tfp 154\n",
      "\t\tfn 259\ttn 2073\n",
      "Masculine recall: 86.4 precision: 93.3 f1: 89.7\n",
      "\t\ttp 768\tfp 55\n",
      "\t\tfn 121\ttn 1056\n",
      "Feminine recall: 84.4 precision: 88.3 f1: 86.3\n",
      "\t\ttp 746\tfp 99\n",
      "\t\tfn 138\ttn 1017\n",
      "Bias (F/M): 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gap_scorer import run_scorer  \n",
    "scores = run_scorer(golden_path, predictions_path)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "with open(\"scores_\"+predictions_path.replace('.tsv',\".txt\"), \"w\") as f:\n",
    "    # json.dump(scores, f, indent=4)\n",
    "    f.writelines(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Cluster\n",
      "0 2\n",
      "['Meg', 'and', 'Vicky']\n",
      "14 14\n",
      "['their']\n",
      "New Cluster\n",
      "0 0\n",
      "['Meg']\n",
      "24 24\n",
      "['Meg']\n",
      "New Cluster\n",
      "2 2\n",
      "['Vicky']\n",
      "26 26\n",
      "['Vicky']\n",
      "39 40\n",
      "['Vicky', 'Austin']\n",
      "50 50\n",
      "['her']\n",
      "[['Meg and Vicky', 'their'], ['Meg', 'Meg'], ['Vicky', 'Vicky', 'Vicky Austin', 'her']]\n"
     ]
    }
   ],
   "source": [
    "clu = []\n",
    "tks = result['document']\n",
    "total_clusters = []\n",
    "for cluster in result['clusters']:\n",
    "    print('New Cluster')\n",
    "    \n",
    "    txt_cluster = []\n",
    "    for start, end in cluster:\n",
    "        print(start,end)\n",
    "        print(tks[start:end+1])\n",
    "        txt_cluster.append(\" \".join(tks[start:end+1]) )\n",
    "    total_clusters.append(txt_cluster)\n",
    "print(total_clusters)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Meg, Start Offset: 0, End Offset: 2, Count 0\n",
      "Token: and, Start Offset: 4, End Offset: 6, Count 1\n",
      "Token: Vicky, Start Offset: 8, End Offset: 12, Count 2\n",
      "Token: each, Start Offset: 14, End Offset: 17, Count 3\n",
      "Token: have, Start Offset: 19, End Offset: 22, Count 4\n",
      "Token: three, Start Offset: 24, End Offset: 28, Count 5\n",
      "Token: siblings, Start Offset: 30, End Offset: 37, Count 6\n",
      "Token: ,, Start Offset: 38, End Offset: 38, Count 7\n",
      "Token: and, Start Offset: 40, End Offset: 42, Count 8\n",
      "Token: have, Start Offset: 44, End Offset: 47, Count 9\n",
      "Token: a, Start Offset: 45, End Offset: 45, Count 10\n",
      "Token: closer, Start Offset: 51, End Offset: 56, Count 11\n",
      "Token: relationship, Start Offset: 58, End Offset: 69, Count 12\n",
      "Token: with, Start Offset: 71, End Offset: 74, Count 13\n",
      "Token: their, Start Offset: 76, End Offset: 80, Count 14\n",
      "Token: youngest, Start Offset: 82, End Offset: 89, Count 15\n",
      "Token: brother, Start Offset: 91, End Offset: 97, Count 16\n",
      "Token: than, Start Offset: 99, End Offset: 102, Count 17\n",
      "Token: with, Start Offset: 104, End Offset: 107, Count 18\n",
      "Token: other, Start Offset: 109, End Offset: 113, Count 19\n",
      "Token: family, Start Offset: 115, End Offset: 120, Count 20\n",
      "Token: members, Start Offset: 122, End Offset: 128, Count 21\n",
      "Token: ., Start Offset: 129, End Offset: 129, Count 22\n",
      "Token: Like, Start Offset: 131, End Offset: 134, Count 23\n",
      "Token: Meg, Start Offset: 136, End Offset: 138, Count 24\n",
      "Token: ,, Start Offset: 139, End Offset: 139, Count 25\n",
      "Token: Vicky, Start Offset: 141, End Offset: 145, Count 26\n",
      "Token: learns, Start Offset: 147, End Offset: 152, Count 27\n",
      "Token: to, Start Offset: 154, End Offset: 155, Count 28\n",
      "Token: silently, Start Offset: 157, End Offset: 164, Count 29\n",
      "Token: communicate, Start Offset: 166, End Offset: 176, Count 30\n",
      "Token: with, Start Offset: 178, End Offset: 181, Count 31\n",
      "Token: a, Start Offset: 183, End Offset: 183, Count 32\n",
      "Token: male, Start Offset: 185, End Offset: 188, Count 33\n",
      "Token: love, Start Offset: 190, End Offset: 193, Count 34\n",
      "Token: interest, Start Offset: 195, End Offset: 202, Count 35\n",
      "Token: via, Start Offset: 204, End Offset: 206, Count 36\n",
      "Token: kything, Start Offset: 208, End Offset: 214, Count 37\n",
      "Token: ., Start Offset: 215, End Offset: 215, Count 38\n",
      "Token: Vicky, Start Offset: 217, End Offset: 221, Count 39\n",
      "Token: Austin, Start Offset: 223, End Offset: 228, Count 40\n",
      "Token: is, Start Offset: 230, End Offset: 231, Count 41\n",
      "Token: about, Start Offset: 233, End Offset: 237, Count 42\n",
      "Token: two, Start Offset: 239, End Offset: 241, Count 43\n",
      "Token: years, Start Offset: 243, End Offset: 247, Count 44\n",
      "Token: older, Start Offset: 249, End Offset: 253, Count 45\n",
      "Token: than, Start Offset: 255, End Offset: 258, Count 46\n",
      "Token: Polly, Start Offset: 260, End Offset: 264, Count 47\n",
      "Token: O'Keefe, Start Offset: 266, End Offset: 272, Count 48\n",
      "Token: ,, Start Offset: 273, End Offset: 273, Count 49\n",
      "Token: her, Start Offset: 275, End Offset: 277, Count 50\n",
      "Token: contemporary, Start Offset: 279, End Offset: 290, Count 51\n",
      "Token: ., Start Offset: 291, End Offset: 291, Count 52\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "char_offsets = []\n",
    "offset = 0\n",
    "token_idx = 0\n",
    "\n",
    "# Iterate over the original text to calculate character-based offsets\n",
    "for i, char in enumerate(text):\n",
    "    # Skip spaces\n",
    "    if char == ' ':\n",
    "        continue\n",
    "    \n",
    "    # Check if the current character matches the start of the next token\n",
    "    if text[i:i+len(result['document'][token_idx])] == result['document'][token_idx]:\n",
    "        start_offset = i\n",
    "        end_offset = i + len(result['document'][token_idx]) - 1\n",
    "        char_offsets.append((start_offset, end_offset))\n",
    "        \n",
    "        # Move the pointer i to the end of the current token\n",
    "        i = end_offset\n",
    "        \n",
    "        # Move to the next token\n",
    "        token_idx += 1\n",
    "        \n",
    "        # Exit the loop if we've found all tokens\n",
    "        if token_idx >= len(result['document']):\n",
    "            break\n",
    "\n",
    "# Debugging: Print each token next to its offset\n",
    "count = 0\n",
    "for (start, end), token in zip(char_offsets, result['document']):\n",
    "    print(f\"Token: {token}, Start Offset: {start}, End Offset: {end}, Count {count}\")\n",
    "    count +=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 'test-1', 'A-coref': False, 'B-coref': True},\n",
       " {'ID': 'test-2', 'A-coref': True, 'B-coref': False},\n",
       " {'ID': 'test-3', 'A-coref': True, 'B-coref': False},\n",
       " {'ID': 'test-4', 'A-coref': False, 'B-coref': True},\n",
       " {'ID': 'test-5', 'A-coref': True, 'B-coref': False}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_annotations = []\n",
    "\n",
    "for index, row in gap_df.iterrows():\n",
    "    gold_annotation = {\n",
    "        'ID': row['ID'],\n",
    "        'A-coref': True if row['A-coref'] == True else False,\n",
    "        'B-coref': True if row['B-coref'] == True else False,\n",
    "        # Add other fields as needed\n",
    "    }\n",
    "    gold_annotations.append(gold_annotation)\n",
    "gold_annotations[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
