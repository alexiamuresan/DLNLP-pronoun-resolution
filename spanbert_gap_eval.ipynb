{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpanBert GAP Evaluation\n",
    "\n",
    "- We load the spanbert model or download it if its missing. \n",
    "- We do data processing for GAP \n",
    "- Generate predictions for GAP data \n",
    "- Use official scorer from GAP repository to compute F1 and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available, predictions will be faster.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from src.gap_scorer import run_scorer  \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# Check for CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available, predictions will be faster.\")\n",
    "else:\n",
    "    print(\"CUDA not available, predictions may be slower.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model or download if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\pc-bae-2\\AppData\\Local\\Temp\\tmpy_0x34aq\\config.json as plain json\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SpanBERT predictor\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Path to the file\n",
    "path_spanbert = 'spanbert_local/'\n",
    "filename = \"coref-spanbert-large-2021.03.10.tar.gz\"\n",
    "\n",
    "\n",
    "save_path = os.path.join(path_spanbert, filename)\n",
    "if os.path.exists(save_path):\n",
    "    predictor = Predictor.from_path(save_path,cuda_device=0)\n",
    "else:\n",
    "        # Create directory if it doesn't exist\n",
    "    if not os.path.exists(path_spanbert):\n",
    "        os.makedirs(path_spanbert)\n",
    "        # Full path to save the file\n",
    "\n",
    "    # Download the file\n",
    "    urllib.request.urlretrieve(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\", save_path)\n",
    "    predictor = Predictor.from_path(save_path,cuda_device=0)\n",
    "\n",
    "    # predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\",cuda_device=0) #use this if urllib didnt work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "Data is in the same folder as notebook so should work as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID                                               Text Pronoun  \\\n",
      "0  test-1  Upon their acceptance into the Kontinental Hoc...     His   \n",
      "1  test-2  Between the years 1979-1981, River won four lo...     him   \n",
      "2  test-3  Though his emigration from the country has aff...      He   \n",
      "3  test-4  At the trial, Pisciotta said: ``Those who have...     his   \n",
      "4  test-5  It is about a pair of United States Navy shore...     his   \n",
      "\n",
      "   Pronoun-offset             A  A-offset  A-coref                   B  \\\n",
      "0             383     Bob Suter       352    False              Dehner   \n",
      "1             430        Alonso       353     True  Alfredo Di St*fano   \n",
      "2             312  Ali Aladhadh       256     True              Saddam   \n",
      "3             526       Alliata       377    False           Pisciotta   \n",
      "4             406         Eddie       421     True         Rock Reilly   \n",
      "\n",
      "   B-offset  B-coref                                             URL  \n",
      "0       366     True      http://en.wikipedia.org/wiki/Jeremy_Dehner  \n",
      "1       390    False    http://en.wikipedia.org/wiki/Norberto_Alonso  \n",
      "2       295    False           http://en.wikipedia.org/wiki/Aladhadh  \n",
      "3       536     True  http://en.wikipedia.org/wiki/Gaspare_Pisciotta  \n",
      "4       559    False            http://en.wikipedia.org/wiki/Chasers  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read GAP data\n",
    "gap_path =  \"data/gap/gap-test.tsv\"\n",
    "# gap_file = \"data/gap/gap-test-gn.tsv\"\n",
    "gap_df = pd.read_csv(gap_path, delimiter='\\t')\n",
    "print(gap_df[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model output example\n",
    "We compute an example and process it to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                                           test-3\n",
      "Text              Though his emigration from the country has aff...\n",
      "Pronoun                                                          He\n",
      "Pronoun-offset                                                  312\n",
      "A                                                      Ali Aladhadh\n",
      "A-offset                                                        256\n",
      "A-coref                                                        True\n",
      "B                                                            Saddam\n",
      "B-offset                                                        295\n",
      "B-coref                                                       False\n",
      "URL                           http://en.wikipedia.org/wiki/Aladhadh\n",
      "Name: 2, dtype: object\n",
      "[[[1, 1], [8, 8], [12, 12]], [[19, 20], [39, 40]], [[32, 34], [42, 47], [49, 50], [61, 61], [65, 65], [69, 69]]]\n",
      "Though his emigration from the country has affected his leadership status, Kamel is still a respected elder of the clan. After the fall of Hussien's regime, many considered Dr. Ali Aladhadh a candidate to lead the clan. A contributor to Iraq's liberation, Ali Aladhadh and a long time oppose to Saddam's regime. He was ambushed with his pregnant wife on his way to the hospital in 2006 by Iraqi insurgents.\n",
      "Though [his] emigration from the country has affected [his] leadership status , [Kamel] is still a respected elder of [the clan] . After the fall of Hussien 's regime , many considered [Dr. Ali Aladhadh] a candidate to lead [the clan] . [A contributor to Iraq 's liberation] , [Ali Aladhadh] and a long time oppose to Saddam 's regime . [He] was ambushed with [his] pregnant wife on [his] way to the hospital in 2006 by Iraqi insurgents .\n"
     ]
    }
   ],
   "source": [
    "# Sample output from AllenNLP coref model\n",
    "example = gap_df.iloc[2]\n",
    "text = example['Text']\n",
    "print(example)\n",
    "result = predictor.predict(document=text)\n",
    "print(result['clusters'])\n",
    "# Initialize an empty list with placeholders\n",
    "text_visual = ['_'] * len(result['document'])\n",
    "\n",
    "# Fill in the placeholders with tokens\n",
    "for i, token in enumerate(result['document']):\n",
    "    text_visual[i] = token\n",
    "\n",
    "# Add brackets for coref clusters\n",
    "for cluster in result['clusters']:\n",
    "    for start, end in cluster:\n",
    "        text_visual[start] = '[' + text_visual[start]\n",
    "        text_visual[end] = text_visual[end] + ']'\n",
    "\n",
    "# Combine into a string\n",
    "text_visual_str = ' '.join(text_visual)\n",
    "print(text)\n",
    "print(text_visual_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 1], [8, 8], [12, 12]], [[19, 20], [39, 40]], [[32, 34], [42, 47], [49, 50], [61, 61], [65, 65], [69, 69]]]\n",
      "[['his', 'his', 'Kamel'], ['the clan', 'the clan'], ['Dr. Ali Aladhadh', \"A contributor to Iraq 's liberation\", 'Ali Aladhadh', 'He', 'his', 'his']]\n"
     ]
    }
   ],
   "source": [
    "# Extract words that correspond to each cluster\n",
    "def extract_cluster_words(tokenized_document, clusters):\n",
    "    total_clusters = []\n",
    "    for cluster in clusters:\n",
    "        txt_cluster = []\n",
    "        for start, end in cluster:\n",
    "            cluster_tokens = tokenized_document[start:end+1]\n",
    "            txt_cluster.append(\" \".join(cluster_tokens))\n",
    "        total_clusters.append(txt_cluster)\n",
    "    return total_clusters\n",
    "\n",
    "# Example \n",
    "tokenized_document = ['It', 'was', 'reported', 'that', 'John', 'and', 'Jane', 'were', 'together', '.', 'He', 'said', 'it', 'was', 'true', '.']\n",
    "clusters = [[[4, 4], [10, 10]], [[5, 5], [12, 12]]]\n",
    "print(result['clusters'])\n",
    "print(extract_cluster_words(result['document'], result['clusters']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation loop\n",
    "Here is where we loop through the data and generate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  83%|████████▎ | 104/125 [01:51<00:21,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Error with selecting clusters, gonna ignore it and continue but be ware\n",
      "list index out of range\n",
      "Warning - Error with selecting clusters, gonna ignore it and continue but be ware\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 125/125 [02:12<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create batches - not working ngl\n",
    "batch_size = 16 \n",
    "num_batches = len(gap_df) // batch_size + (1 if len(gap_df) % batch_size != 0 else 0)\n",
    "\n",
    "predictions = []\n",
    "num_rows = len(gap_df)\n",
    "num_batches = (num_rows + batch_size - 1) // batch_size\n",
    "for batch_idx in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, num_rows)\n",
    "        batch = gap_df.iloc[start_idx:end_idx]\n",
    "        for _, row in  batch.iterrows():\n",
    "            # print(row)\n",
    "            text = row['Text']\n",
    "            result = predictor.predict(document=text)\n",
    "            clusters = result['clusters']\n",
    "            tokens = result['document']\n",
    "\n",
    "            # Initialize coreference indicators for A and B to 0\n",
    "            a_coref, b_coref = 0, 0\n",
    "\n",
    "            # Find the cluster containing the pronoun\n",
    "            pronoun_offset = row['Pronoun-offset']\n",
    "            pronoun_length = len(row['Pronoun'])\n",
    "            pronoun_cluster = None\n",
    "\n",
    "            # Calculate the character offsets for each token\n",
    "            char_offsets = []\n",
    "            offset = 0\n",
    "            token_idx = 0\n",
    "            \n",
    "            tokens_dict = {}\n",
    "            for i, char in enumerate(text):\n",
    "                if char == ' ':\n",
    "                    continue\n",
    "                if text[i:i+len(tokens[token_idx])] == tokens[token_idx]:\n",
    "                    start_offset = i\n",
    "                    end_offset = i + len(tokens[token_idx]) - 1\n",
    "                    char_offsets.append((start_offset, end_offset))\n",
    "                    tokens_dict[ (start_offset, end_offset)] =tokens[token_idx]\n",
    "\n",
    "                    i = end_offset\n",
    "                    token_idx += 1\n",
    "                    if token_idx >= len(tokens):\n",
    "                        break\n",
    "                    \n",
    "            # print(tokens_dict)\n",
    "            # print('clusters',clusters)\n",
    "            # clusters = sorted(clusters, key=len) #Sort base on how cluster size, smaller is more important!\n",
    "            \n",
    "            # print('sorted clusters',clusters)\n",
    "            pronoun_clusters = []\n",
    "            for cluster in clusters:\n",
    "                for start, end in cluster:\n",
    "                    # print(char_offsets[start])\n",
    "                    try:\n",
    "                        start_offset, end_offset = char_offsets[start]\n",
    "                        if start_offset <= pronoun_offset and end_offset >= (pronoun_offset + pronoun_length - 1):\n",
    "                            pronoun_cluster = cluster\n",
    "                            # print(\"Pronoun cluster:\",cluster)\n",
    "                            pronoun_clusters.append(pronoun_cluster)\n",
    "                        # break\n",
    "                    except Exception as e:\n",
    "                        print(\"Warning - Error with selecting clusters, gonna ignore it and continue but be ware\")\n",
    "                        print(e)\n",
    "                        continue\n",
    "                # if pronoun_cluster:\n",
    "                #     break\n",
    "            # print(extract_cluster_words(tokens,[pronoun_cluster]))\n",
    "            # Check if 'A' or 'B' is in the same cluster as the pronoun\n",
    "            for pronoun_cluster in pronoun_clusters:\n",
    "                a_start, a_end = row['A-offset'], row['A-offset'] + len(row['A']) - 1\n",
    "                b_start, b_end = row['B-offset'], row['B-offset'] + len(row['B']) - 1\n",
    "                # print(\"A goal:\",a_start,a_end)\n",
    "                # print(\"B goal:\",b_start,b_end)\n",
    "                for start, end in pronoun_cluster:\n",
    "                    # print(\"start,end\",start,end)\n",
    "                    start_offset, _ = char_offsets[start]\n",
    "                    _, end_offset =char_offsets[end]\n",
    "                    # print(\"start,end char:\", start_offset, end_offset)\n",
    "                    if start_offset <= a_start and end_offset >= a_end and b_coref == 0:\n",
    "                        a_coref = 1\n",
    "                        break\n",
    "                    if start_offset <= b_start and end_offset >= b_end and a_coref == 0:\n",
    "                        b_coref = 1\n",
    "                        break\n",
    "\n",
    "            predictions.append({\n",
    "                'ID': row['ID'],\n",
    "                'A-coref': a_coref,\n",
    "                'B-coref': b_coref\n",
    "            })\n",
    "            # print(predictions)\n",
    "            # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scores\n",
    "Here we compute the f1 scores using gap_scorer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "golden_path = gap_path #this should match with the file we used to generate predictions on top of notebook\n",
    "# golden_path = 'data/gap/gap-development.tsv'\n",
    "# golden_path = 'data/gap/gap-validation.tsv'\n",
    "golden_path = 'data/gap/gap-test.tsv' \n",
    "predictions_path = 'outputs/spanbert/predictions-test.tsv'\n",
    "os.makedirs(os.path.dirname(predictions_path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID  A-coref  B-coref\n",
      "0  test-1        0        1\n",
      "1  test-2        1        0\n",
      "2  test-3        1        0\n",
      "3  test-4        0        1\n",
      "4  test-5        1        0\n"
     ]
    }
   ],
   "source": [
    "# dictnary to df\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "print(predictions_df[:5])\n",
    "# Convert 1 to True and 0 to False in the A-coref and B-coref columns\n",
    "predictions_df['A-coref'] = predictions_df['A-coref'].astype(bool)\n",
    "predictions_df['B-coref'] = predictions_df['B-coref'].astype(bool)\n",
    "# DataFrame to a TSV file\n",
    "predictions_df.to_csv(predictions_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected label! A-coref\n",
      "Unexpected label! B-coref\n",
      "Overall recall: 85.4 precision: 90.8 f1: 88.0\n",
      "\t\ttp 1514\tfp 154\n",
      "\t\tfn 259\ttn 2073\n",
      "Masculine recall: 86.4 precision: 93.3 f1: 89.7\n",
      "\t\ttp 768\tfp 55\n",
      "\t\tfn 121\ttn 1056\n",
      "Feminine recall: 84.4 precision: 88.3 f1: 86.3\n",
      "\t\ttp 746\tfp 99\n",
      "\t\tfn 138\ttn 1017\n",
      "Bias (F/M): 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gap_scorer import run_scorer  \n",
    "predictions_path = 'outputs/spanbert/predictions-test.tsv'\n",
    "scores = run_scorer(golden_path, predictions_path)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "with open(\"scores_\"+predictions_path.replace('.tsv',\".txt\"), \"w\") as f:\n",
    "    # json.dump(scores, f, indent=4)\n",
    "    f.writelines(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B3_0(mentions, true_clusters, pred_clusters):\n",
    "  '''\n",
    "  Calculates precision, recall, and optionally F1 for the  B3(0) metric,\n",
    "  based on formulation in https://aclanthology.org/W10-4305.pdf\n",
    "\n",
    "  returns precision, recall and f1 as lists for the input sentence\n",
    "  '''\n",
    "\n",
    "  precision_scores = []\n",
    "  recall_scores = []\n",
    "  f1_scores = []\n",
    "\n",
    "  for mention in mentions:\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "\n",
    "    # finding key and response clusters to look at (first cluster to come up that contains current mention)\n",
    "    mention_key_cluster = None\n",
    "    for cluster in true_clusters:\n",
    "      if mention in cluster:\n",
    "        mention_key_cluster = cluster\n",
    "        break\n",
    "    assert mention_key_cluster, \"At least one true cluster must contain mention!\"\n",
    "\n",
    "    mention_pred_cluster = None\n",
    "    for cluster in pred_clusters:\n",
    "      if mention in cluster:\n",
    "        mention_response_cluster = cluster\n",
    "        break\n",
    "\n",
    "    intersection_key_response = list((Counter(mention_key_cluster) & Counter(mention_response_cluster)).elements())\n",
    "    overlap_count = len(intersection_key_response)\n",
    "\n",
    "    # response cluster could be empty if mention was not predicted for any cluster (twinless mention); in this case precision and recall both at 0\n",
    "    if mention_response_cluster:\n",
    "      precision = overlap_count / len(mention_response_cluster)\n",
    "      recall = overlap_count / len(mention_key_cluster)\n",
    "\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "# Check for zero division\n",
    "    if precision + recall != 0:\n",
    "        f1_scores.append((2 * precision * recall) / (precision + recall))\n",
    "    else:\n",
    "        f1_scores.append(0.0)\n",
    "  return precision_scores, recall_scores, f1_scores\n",
    "def global_B3_0(precision_scores, recall_scores, F1_scores):\n",
    "  '''\n",
    "  Calculates global precision, recall and F1 scores based on lists of\n",
    "  individual B3_0 precision/recall/F1 scores per mention\n",
    "  '''\n",
    "\n",
    "  B3_0_precision = sum(precision_scores)/len(precision_scores)\n",
    "  B3_0_recall = sum(recall_scores)/len(recall_scores)\n",
    "  B3_0_F1 = sum(F1_scores)/len(F1_scores)\n",
    "\n",
    "  return B3_0_precision, B3_0_recall, B3_0_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dummy GAP DataFrame\n",
    "gap_df = pd.DataFrame({\n",
    "    'ID': ['test-1', 'test-2'],\n",
    "    'Pronoun': ['he', 'she'],\n",
    "    'A': ['John', 'Anna'],\n",
    "    'B': ['Mike', 'Emily']\n",
    "})\n",
    "\n",
    "# Dummy Predictions DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ID': ['test-1', 'test-2'],\n",
    "    'A-coref': [False, True],\n",
    "    'B-coref': [True, False]\n",
    "})\n",
    "\n",
    "# Merge the two DataFrames on the \"ID\" column\n",
    "merged_df = pd.merge(gap_df, predictions_df, on='ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clusters_and_mentions(merged_df):\n",
    "    # lists to store true and predicted clusters, and mentions for each sentence\n",
    "    true_clusters_list = []\n",
    "    pred_clusters_list = []\n",
    "    mentions_list = []\n",
    "    gender_list = []\n",
    "    # Iterate through the merged DataFrame to generate clusters and mentions\n",
    "    for _, row in merged_df.iterrows():\n",
    "        pronoun = row['Pronoun']\n",
    "        option_a = row['A']\n",
    "        option_b = row['B']\n",
    "        a_coref = row['A-coref']\n",
    "        b_coref = row['B-coref']\n",
    "\n",
    "        # mentions for this instance\n",
    "        mentions = [pronoun, option_a, option_b]\n",
    "\n",
    "        # true clusters \n",
    "        true_cluster = [[pronoun, option_a], [option_b]]\n",
    "        gendered_pronouns = row['Pronoun_old'] if 'Pronoun_old' in merged_df.columns else row['Pronoun']\n",
    "        gendered_pronouns = gendered_pronouns.lower()\n",
    "        if gendered_pronouns == 'he' or gendered_pronouns == 'his' or gendered_pronouns == 'him':\n",
    "            gender_list.append('M')\n",
    "        elif gendered_pronouns == 'she' or gendered_pronouns == 'her' or gendered_pronouns == 'hers':\n",
    "             gender_list.append('F')\n",
    "        # pred clusters based on model output\n",
    "        pred_cluster = []\n",
    "        if a_coref:\n",
    "            pred_cluster.append([pronoun, option_a])\n",
    "        elif b_coref:\n",
    "            pred_cluster.append([pronoun, option_b])\n",
    "        else:\n",
    "            pred_cluster.append([pronoun])\n",
    "\n",
    "        true_clusters_list.append(true_cluster)\n",
    "        pred_clusters_list.append(pred_cluster)\n",
    "        mentions_list.append(mentions)\n",
    "\n",
    "    return true_clusters_list, pred_clusters_list, mentions_list, gender_list\n",
    "\n",
    "\n",
    "true_clusters_list, pred_clusters_list, mentions_list, gender_list = generate_clusters_and_mentions(merged_df)\n",
    "\n",
    "# run B3_0 function for each sentence\n",
    "for mentions, true_clusters, pred_clusters in zip(mentions_list, true_clusters_list, pred_clusters_list):\n",
    "    precision_scores, recall_scores, f1_scores = B3_0(mentions, true_clusters, pred_clusters)\n",
    "    print(f\"For mentions {mentions}:\")\n",
    "    print(f\"Precision: {precision_scores}, Recall: {recall_scores}, F1: {f1_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B3_0(mentions, true_clusters, pred_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_file = \"data/gap/gap-test.tsv\"\n",
    "predictions_file = \"outputs/spanbert/predictions-test.tsv\"\n",
    "# gap_file = \"data/gap/gap-test-gn.tsv\"\n",
    "# predictions_file = \"outputs/spanbert/predictions-test-gn.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading GAP DataFrame from a TSV file\n",
    "gap_df = pd.read_csv(gap_file, sep='\\t')\n",
    "gap_df.rename(columns={'A-coref': 'gold_A-coref', 'B-coref': 'gold_B-coref'},inplace=True)# Reading Predictions DataFrame from a TSV file\n",
    "predictions_df = pd.read_csv(predictions_file, sep='\\t')\n",
    "\n",
    "# Merge the two DataFrames on the \"ID\" column\n",
    "merged_df = pd.merge(gap_df, predictions_df, on='ID')\n",
    "\n",
    "# Display the first few rows to check\n",
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_clusters_list, pred_clusters_list, mentions_list, gender_list = generate_clusters_and_mentions(merged_df)\n",
    "# B3_0(mentions_list, true_clusters_list, pred_clusters_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_precision_scores = []\n",
    "total_recall_scores = []\n",
    "total_f1_scores = []\n",
    "male_precision_scores = []\n",
    "male_recall_scores = []\n",
    "male_f1_scores = []\n",
    "\n",
    "female_precision_scores = []\n",
    "female_recall_scores = []\n",
    "female_f1_scores = []\n",
    "\n",
    "for mentions, true_clusters, pred_clusters, gender in zip(mentions_list, true_clusters_list, pred_clusters_list, gender_list):\n",
    "    # print(f\"Mentions {mentions}, true clusters {true_clusters}, predicted clusters {pred_clusters}\")\n",
    "    try:\n",
    "        precision_scores, recall_scores, f1_scores = B3_0(mentions, true_clusters, pred_clusters)\n",
    "        total_precision_scores += precision_scores\n",
    "        total_recall_scores += recall_scores\n",
    "        total_f1_scores += f1_scores\n",
    "\n",
    "        if gender == 'M':\n",
    "            male_precision_scores.extend(precision_scores)\n",
    "            male_recall_scores.extend(recall_scores)\n",
    "            male_f1_scores.extend(f1_scores)\n",
    "        elif gender == 'F':\n",
    "            female_precision_scores.extend(precision_scores)\n",
    "            female_recall_scores.extend(recall_scores)\n",
    "            female_f1_scores.extend(f1_scores)\n",
    "    except:\n",
    "        print(f\"Mentions {mentions}, true clusters {true_clusters}, predicted clusters {pred_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_precision, global_recall, global_f1 = global_B3_0(total_precision_scores, total_recall_scores, total_f1_scores)\n",
    "\n",
    "# format the results \n",
    "results_str = f\"{gap_file} Global B3 Scores:\\n\"\n",
    "results_str += f\"Precision: {global_precision:.2f}\\n\"\n",
    "results_str += f\"Recall: {global_recall:.2f}\\n\"\n",
    "results_str += f\"F1 Score: {global_f1:.2f}\"\n",
    "\n",
    "print(results_str)\n",
    "\n",
    "# save results to a text file\n",
    "with open(gap_file+\"_global_B3_scores.txt\", \"w\") as f:\n",
    "    f.write(results_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# global scores\n",
    "global_male_precision, global_male_recall, global_male_f1 = global_B3_0(male_precision_scores, male_recall_scores, male_f1_scores)\n",
    "global_female_precision, global_female_recall, global_female_f1 = global_B3_0(female_precision_scores, female_recall_scores, female_f1_scores)\n",
    "\n",
    "#F/M ratio for each metric\n",
    "precision_ratio = global_female_precision / global_male_precision\n",
    "recall_ratio = global_female_recall / global_male_recall\n",
    "f1_ratio = global_female_f1 / global_male_f1\n",
    "\n",
    "results_str = f\"Global B3 Scores:\\n\"\n",
    "results_str += f\"Male Precision: {global_male_precision:.2f}, Female Precision: {global_female_precision:.2f}, Precision Ratio (F/M): {precision_ratio:.2f}\\n\"\n",
    "results_str += f\"Male Recall: {global_male_recall:.2f}, Female Recall: {global_female_recall:.2f}, Recall Ratio (F/M): {recall_ratio:.2f}\\n\"\n",
    "results_str += f\"Male F1: {global_male_f1:.2f}, Female F1: {global_female_f1:.2f}, F1 Ratio (F/M): {f1_ratio:.2f}\"\n",
    "\n",
    "print(results_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Cluster\n",
      "0 2\n",
      "['Meg', 'and', 'Vicky']\n",
      "14 14\n",
      "['their']\n",
      "New Cluster\n",
      "0 0\n",
      "['Meg']\n",
      "24 24\n",
      "['Meg']\n",
      "New Cluster\n",
      "2 2\n",
      "['Vicky']\n",
      "26 26\n",
      "['Vicky']\n",
      "39 40\n",
      "['Vicky', 'Austin']\n",
      "50 50\n",
      "['her']\n",
      "[['Meg and Vicky', 'their'], ['Meg', 'Meg'], ['Vicky', 'Vicky', 'Vicky Austin', 'her']]\n"
     ]
    }
   ],
   "source": [
    "clu = []\n",
    "tks = result['document']\n",
    "total_clusters = []\n",
    "for cluster in result['clusters']:\n",
    "    print('New Cluster')\n",
    "    \n",
    "    txt_cluster = []\n",
    "    for start, end in cluster:\n",
    "        print(start,end)\n",
    "        print(tks[start:end+1])\n",
    "        txt_cluster.append(\" \".join(tks[start:end+1]) )\n",
    "    total_clusters.append(txt_cluster)\n",
    "print(total_clusters)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Meg, Start Offset: 0, End Offset: 2, Count 0\n",
      "Token: and, Start Offset: 4, End Offset: 6, Count 1\n",
      "Token: Vicky, Start Offset: 8, End Offset: 12, Count 2\n",
      "Token: each, Start Offset: 14, End Offset: 17, Count 3\n",
      "Token: have, Start Offset: 19, End Offset: 22, Count 4\n",
      "Token: three, Start Offset: 24, End Offset: 28, Count 5\n",
      "Token: siblings, Start Offset: 30, End Offset: 37, Count 6\n",
      "Token: ,, Start Offset: 38, End Offset: 38, Count 7\n",
      "Token: and, Start Offset: 40, End Offset: 42, Count 8\n",
      "Token: have, Start Offset: 44, End Offset: 47, Count 9\n",
      "Token: a, Start Offset: 45, End Offset: 45, Count 10\n",
      "Token: closer, Start Offset: 51, End Offset: 56, Count 11\n",
      "Token: relationship, Start Offset: 58, End Offset: 69, Count 12\n",
      "Token: with, Start Offset: 71, End Offset: 74, Count 13\n",
      "Token: their, Start Offset: 76, End Offset: 80, Count 14\n",
      "Token: youngest, Start Offset: 82, End Offset: 89, Count 15\n",
      "Token: brother, Start Offset: 91, End Offset: 97, Count 16\n",
      "Token: than, Start Offset: 99, End Offset: 102, Count 17\n",
      "Token: with, Start Offset: 104, End Offset: 107, Count 18\n",
      "Token: other, Start Offset: 109, End Offset: 113, Count 19\n",
      "Token: family, Start Offset: 115, End Offset: 120, Count 20\n",
      "Token: members, Start Offset: 122, End Offset: 128, Count 21\n",
      "Token: ., Start Offset: 129, End Offset: 129, Count 22\n",
      "Token: Like, Start Offset: 131, End Offset: 134, Count 23\n",
      "Token: Meg, Start Offset: 136, End Offset: 138, Count 24\n",
      "Token: ,, Start Offset: 139, End Offset: 139, Count 25\n",
      "Token: Vicky, Start Offset: 141, End Offset: 145, Count 26\n",
      "Token: learns, Start Offset: 147, End Offset: 152, Count 27\n",
      "Token: to, Start Offset: 154, End Offset: 155, Count 28\n",
      "Token: silently, Start Offset: 157, End Offset: 164, Count 29\n",
      "Token: communicate, Start Offset: 166, End Offset: 176, Count 30\n",
      "Token: with, Start Offset: 178, End Offset: 181, Count 31\n",
      "Token: a, Start Offset: 183, End Offset: 183, Count 32\n",
      "Token: male, Start Offset: 185, End Offset: 188, Count 33\n",
      "Token: love, Start Offset: 190, End Offset: 193, Count 34\n",
      "Token: interest, Start Offset: 195, End Offset: 202, Count 35\n",
      "Token: via, Start Offset: 204, End Offset: 206, Count 36\n",
      "Token: kything, Start Offset: 208, End Offset: 214, Count 37\n",
      "Token: ., Start Offset: 215, End Offset: 215, Count 38\n",
      "Token: Vicky, Start Offset: 217, End Offset: 221, Count 39\n",
      "Token: Austin, Start Offset: 223, End Offset: 228, Count 40\n",
      "Token: is, Start Offset: 230, End Offset: 231, Count 41\n",
      "Token: about, Start Offset: 233, End Offset: 237, Count 42\n",
      "Token: two, Start Offset: 239, End Offset: 241, Count 43\n",
      "Token: years, Start Offset: 243, End Offset: 247, Count 44\n",
      "Token: older, Start Offset: 249, End Offset: 253, Count 45\n",
      "Token: than, Start Offset: 255, End Offset: 258, Count 46\n",
      "Token: Polly, Start Offset: 260, End Offset: 264, Count 47\n",
      "Token: O'Keefe, Start Offset: 266, End Offset: 272, Count 48\n",
      "Token: ,, Start Offset: 273, End Offset: 273, Count 49\n",
      "Token: her, Start Offset: 275, End Offset: 277, Count 50\n",
      "Token: contemporary, Start Offset: 279, End Offset: 290, Count 51\n",
      "Token: ., Start Offset: 291, End Offset: 291, Count 52\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "char_offsets = []\n",
    "offset = 0\n",
    "token_idx = 0\n",
    "\n",
    "# Iterate over the original text to calculate character-based offsets\n",
    "for i, char in enumerate(text):\n",
    "    # Skip spaces\n",
    "    if char == ' ':\n",
    "        continue\n",
    "    \n",
    "    # Check if the current character matches the start of the next token\n",
    "    if text[i:i+len(result['document'][token_idx])] == result['document'][token_idx]:\n",
    "        start_offset = i\n",
    "        end_offset = i + len(result['document'][token_idx]) - 1\n",
    "        char_offsets.append((start_offset, end_offset))\n",
    "        \n",
    "        # Move the pointer i to the end of the current token\n",
    "        i = end_offset\n",
    "        \n",
    "        # Move to the next token\n",
    "        token_idx += 1\n",
    "        \n",
    "        # Exit the loop if we've found all tokens\n",
    "        if token_idx >= len(result['document']):\n",
    "            break\n",
    "\n",
    "# Debugging: Print each token next to its offset\n",
    "count = 0\n",
    "for (start, end), token in zip(char_offsets, result['document']):\n",
    "    print(f\"Token: {token}, Start Offset: {start}, End Offset: {end}, Count {count}\")\n",
    "    count +=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 'test-1', 'A-coref': False, 'B-coref': True},\n",
       " {'ID': 'test-2', 'A-coref': True, 'B-coref': False},\n",
       " {'ID': 'test-3', 'A-coref': True, 'B-coref': False},\n",
       " {'ID': 'test-4', 'A-coref': False, 'B-coref': True},\n",
       " {'ID': 'test-5', 'A-coref': True, 'B-coref': False}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_annotations = []\n",
    "\n",
    "for index, row in gap_df.iterrows():\n",
    "    gold_annotation = {\n",
    "        'ID': row['ID'],\n",
    "        'A-coref': True if row['A-coref'] == True else False,\n",
    "        'B-coref': True if row['B-coref'] == True else False,\n",
    "        # Add other fields as needed\n",
    "    }\n",
    "    gold_annotations.append(gold_annotation)\n",
    "gold_annotations[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
